\documentclass{amsart}
\usepackage[foot]{amsaddr} % put addresses on first page

\usepackage{geometry}
\usepackage{booktabs}
\usepackage{graphicx,psfrag,epsf}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{amsmath}
\allowdisplaybreaks
\usepackage{longtable}
\usepackage{bigints}
\usepackage{siunitx}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{amsthm}
\usepackage{soul}
\usepackage{makecell}
\usepackage{color}
\usepackage{hyperref}
\usepackage[capitalise]{cleveref}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{definition}{Definition}[section]
%

\usepackage[authoryear, round]{natbib}
\usepackage{url} % not crucial - just used below for the URL 
\usepackage{doi}

\newcommand{\dashrule}{\hdashline\noalign{\vskip 0.5ex}}
\renewcommand\theadalign{l}
\renewcommand\theadfont{}
\renewcommand\theadgape{}

\title{Conditional Copula models using loss-based Bayesian Additive Regression Trees}
\author{Tathagata Basu$^1$}
\author{Fabrizio Leisen$^2$}
\author{Cristiano Villa$^3$}
\author{Kevin Wilson$^1$}
\address{$^1$Newcastle University, UK}
\address{$^2$Kings College London, UK}
\address{$^3$Duke Kunshan University, China}

\date{\today}

	\begin{document}

\begin{abstract}
We present a novel semi-parametric Bayesian approach for modelling conditional copulas to understand the dependence structure between two random variables when it is influenced by an external factor. We use a Bayesian Additive Regression Trees (BART) model along with recently proposed loss-based priors, which recovers the true parsimonious tree structures and reduces overfitting — a common issue in classical BART models. We introduce a novel adaptive Reversible Jump Markov Chain Monte Carlo (RJ-MCMC) type algorithm that automatically tunes the proposal variance, giving us a greater flexibility in analysing models with non-trivial or non-smooth likelihood funtion. We test the efficiency of our model using simulated datasets. We show that our method can efficiently recover the true tree structure and approximate a complex conditional copula parameter, as well as the efficiency of our adaptive routine in exploring the true likelihood region under sub-optimal proposal variance. Lastly, we exhibit the applicability of our method using a real dataset involving the effect of gross domestic product on the dependence between the life expectancy and literacy rates of the male and female population of different countries.
\end{abstract}

\keywords{Conditional Copula; BART; Objective Bayes; Semi-parametric estimation}

\maketitle

\section{Introduction}
In multivariate analysis, we often observe that random variables exhibit a dependence in their behaviour and the study of such dependence is extremely important due to its implications in real-life modelling. However, analysing such dependence structures can be very cumbersome as we often require non-trivial multivariate distributions which makes this problem extremely challenging. Sklar's theorem\citep{sklar:1959} simplified such tasks, which states that a multivariate distribution can be modelled using it's marginals and a copula, that is, a joint cumulative distribution function on the unit hypercube. Following this, several other theoretical advancements \citep{kimeldori1975uniform,ruschendorf1976,schweizer1981,Genest01091993,Genest1993mult} in the field, made copula inference popular in the context of statistical modelling leading to its prominence in applied statistical literature involving  survival analysis \citep 	{clayton1978model,oakes1989bivariate,zheng1995estimates,shih1995inferences,braekers2005copula}; risk management \citep{fama1993common,BURGERT2006289,engle1990asset}; engineering applications \citep{salvadori2007use,aghakouchak2010copula}; genetics \citep{li2006quantitative} etc. While copula modelling remained popular in multivariate analyses, a major computational improvement can be credited to \citet{bedford_vine_2002}, where they introduced a nested graphical model for copula construction and coined it as `Vine Copula'. Later, based on their work \citet{czado_pair_cop_2010} introduced pair wise copula for multivariate modelling which improved the computational aspect further.  We refer readers to \citet{GENEST2024105278}'s review on copula modelling in remembrance of Abe Sklar for a detailed literature review.

In many situations, the dependence between random variables can have an external influence, in such cases it is useful to adjust for external covariates in copulas. To tackle this issue, \citet{patton2006} formalised the conditional version of Sklar's theorem for its applicability in financial time series modelling. Initial works on conditional copulas were mostly for estimating time varying dependence structures using likelihood based approaches for autoregressive models \citep{patton2006,JONDEAU2006827,BARTRAM20071461}. Later \citet{acar2010} proposed a non-parametric approach for estimating conditional copulas for general problems. Several other likelihood-based non-parametric \citep{GIJBELS20111919,Gijbels2012mult_cop} and semi-parametric \citep{ABEGAZ201243} approaches have been proposed in this regard. \citet{valle_cond_cop} proposed a Dirichlet mixture models for estimating conditional copulas and \citet{GRAZIAN2022107417} proposed an approximate Bayesian approach for the same. Recently, \citet{BonacinaLopezThomas+2025} proposed a ``Classification and Regression Tree'' (CART) based \citep{brei_CART} algorithm for modelling conditional copulas and investigated the consistency of the CART algorithm in conditional copula estimation.

The introduction of the CART algorithm for conditional copula estimation motivates us to explore the viability of employing Bayesian additive Regression Trees (BART), the Bayesian alternative of CART introduced by \citet{chipman2010BART}. BART provides a generalisation of earlier Bayesian CART models \citep{chipman98BCART,denison98BCART} where a single regression tree was used. Due to the flexible nature of BART, it has been explored in different contexts such as Poisson regression \citep{Murray03042021}; survival analysis \citep{Sparapani_BART}; gamma regression \citep{Linero_BART_gamma}; generalised BART \citep{Linero02012025}. While the original tree prior proposed by \citet{chipman98BCART} is most commonly used in BART literatures, it is not very straightforward to incorporate prior information on the number of terminal nodes. On the other hand the prior proposed by \citet{denison98BCART} tends to produce skewed trees. Several alternatives are proposed \citep{Wu_CART,rockova_BART,Linero_BART_VS} to tackle this issue but choice of hyperparameters remains subjective. In order to reduce such subjectivity, \citet{serafini2024lossbasedpriortreetopologies} proposed a novel prior for regression trees using a loss based approach developed by \citet{villa_loss-prior}. The prior is formulated based on minimising the loss incurred due to the misspecification of the tree, which considers both the loss in information and complexity of the tree, making it appealing for BART models.

In this article, we exploit the loss-based BART prior developed by \citet{serafini2024lossbasedpriortreetopologies} to propose a novel approach for modelling conditional copulas. However, unlike previous applications of BART models, we do not have a straight forward likelihood function to chose a conjugate prior. Therefore, we opt for a trans-dimensional MCMC algorithm that can sample a new tree of a different dimension as well as sample new terminal node values. Similar method, proposed by \citet{Linero02012025} uses a Laplace approximation based approach for sampling terminal node values. Unfortunately, Laplace approximation is not viable for conditional copula modelling as the first and second order derivatives of the likelihood may not be numerically stable. To overcome this issue, we propose an efficient reversible jump MCMC (RJ-MCMC) type algorithm \citep{green_RJMCMC} to sample from the posterior. However, we notice that choosing the parameters of the proposal distribution is rather difficult task for the sum of trees models and mixing can be extremely slow in some cases. This motivates us to introduce an adaptive formulation for the choice of proposal distribution which efficiently updates the variance of a Gaussian proposal using previously accumulated MCMC samples. Initial analyses shows that our adaptive approach also converges faster to higher likelihood regions. We illustrate our method using different copula models to show the efficiency of our approach in identifying the true tree structure, as well as estimating the true conditional dependence. Furthermore, due to its simple and explicable implementation, it can easily be adapted for other modelling problems using BART. 

The rest of the paper is organised as follows: in \cref{sec:prelim} we introduce preliminary concepts of conditional copulas and Bayesian additive regression trees, followed by our semi-parametric estimation approach for conditional copulas in \cref{sec:cond:cop}. In \cref{sec:rjmcmc}, we discuss our proposed reversible jump MCMC routine for sampling from the posterior, along with its adaptive variant. After that, we illustrate our method using two synthetic datasets in \cref{sec:sim} to show its efficiency and accuracy; followed by case studies using CIA world fact dataset in \cref{sec:cia}. Finally, we discuss the results shown in the paper and conclude it in \cref{sec:conc}.

\section{Preliminaries}\label{sec:prelim}

In this section, we present a formal description of conditional copulas followed by the BART models and the loss-based prior for BART proposed by \citet{serafini2024lossbasedpriortreetopologies}, which we will incorporate for modelling.

\subsection{Conditional copula}
Let $Y_1$ and $Y_2$ denote two random variables such that they have continuous marginal cumulative distribution functions $F_1(y_1)$ and $F_2(y_2)$. Then according to \citet{sklar:1959}, we can use a copula $C$ with parameter $\theta$ to model the dependence between $Y_1$ and $Y_2$ so that
\begin{equation*}
	H(y_1,y_2) = C\left(F_{1}(y_1),F_{2}(y_2)\mid \theta\right)
\end{equation*}
where $H(y_1,y_2)$ is the joint cumulative distribution function of $Y_1$ and $Y_2$.  

In many practical situations, the dependence between $Y_1$ and $Y_2$ can be influenced by an external variable $X$. To tackle such issues, \citet{patton2006} suggested a conditional version of \citet{sklar:1959}'s theorem. Let, $H_x(y_1,y_2)$ denote joint cumulative distribution of $Y_1$ and $Y_2$ and $F_{ix}(y_i)$ is the marginal of $Y_i$ conditional on $X$ for $i=1,2$. Then we can write the following
\begin{equation*}
	H_x(y_1,y_2) = C\left(F_{1x}(y_1),F_{2x}(y_2)\mid \theta(x)\right)
\end{equation*}
where $\theta(x)$ is the copula parameter conditional on $x$. Alternatively, let $u_i = F_{ix}(y_i)$ be the pseudo observations and $F_{ix}^{-1}(u_i)$s be the conditional quantile functions
for $i=1,2$. Then alternate formulation holds as well:
\begin{equation*}
	C\left(u_1,u_2\mid \theta(x)\right) = H_x\left(F_{1x}^{-1}(u_1),F_{2x}^{-1}(u_2)\right).
\end{equation*}
For a more detailed introduction to the concept check \citet{patton2006,acar2010,GIJBELS20111919} etc.

\subsection{Loss-based BART}

Before defining the loss-based prior for BART, we first discuss regression tree models. Let $T$ denote a regression tree with a set of internal nodes depicting decision rules and a set of terminal nodes that assigns functional values corresponding to decision rules. We denote this set of terminal node values by $M =$ $\{\mu_1$,$\mu_2$, \dots, $\mu_{n_L(T)}\}$ where $n_L(T)$ is the number of terminal nodes of the tree $T$. Then we can predict an output $Z\coloneqq(Z_1,\cdots,Z_n)$ in the following way:
\begin{equation*}
	Z_i = g(x_i, T, M) + \epsilon_i; \qquad \epsilon_i\sim \mathcal{N}(0,\sigma^2)
\end{equation*} 
where $g$ is a function that assigns a terminal node value to $x_i$ for $i=1,\cdots,n$ and $\epsilon_i$ is the associated noise that follows a normal distribution with variance $\sigma^2$. This is a single tree model developed by \citet{chipman98BCART}, this is extended to a sum of trees model by \citet{chipman2010BART} in the following way:
\begin{equation}\label{eq:BART}
	Z_i = \sum_{t=1}^m g(x_i, T_t, M_t) + \epsilon_i; \qquad \epsilon_i\sim \mathcal{N}(0,\sigma^2)
\end{equation}
where $T_t$ denotes the $t$-th tree; $M_t$ denotes the $t$-th vector of terminal node values $M_t =$ $\{\mu_1$,$\mu_2$, \dots, $\mu_{n_L(T_t)}\}$ of the $t$-th tree; and $n_L(T_t)$ denotes the number of terminal nodes of the $t$-th tree.

This representation allows us to approximate a function with piecewise constant values on a partitioned domain. These partitions are constructed by assigning a splitting rule $x_{\cdot j}\le \kappa$ for $1\le j \le p$ on each internal node, where the value $\kappa$ is either chosen from one of the observed values $x_{ij}$ or chosen uniformly within a range of values $(\underline{x}_{\cdot j},\overline{x}_{\cdot j})$.

The most popular choice for a tree prior was introduced by \citet{chipman98BCART}, which later was used by \citet{chipman2010BART} for sum of regression trees models. Recently, \citet{serafini2024lossbasedpriortreetopologies} proposed an alternate prior for the tree topology. They consider a loss-based approach, which allows us to design objective priors \citep{villa_loss-prior}. They suggested that the associated loss function for misspecification of a tree has two components: loss in information and loss in complexity. Further, they showed that the loss in complexity can be defined as $\omega n_L(T_t) - \zeta \Delta(T_t)$, where $\omega\ge 0$ and $\zeta\in\mathbb{R}$ are prior parameters; $\Delta(T_t)$ is the difference between the right terminal nodes and the left terminal nodes of $t$-th given tree; $n_L(T_t)$ is number of terminal nodes of the $t$-th tree. This leads to the following hierarchical prior

\begin{align}\label{eq:L-BART}
	\begin{split}
		T_t, M_t &\sim \pi(T_t)\pi(M_t\mid T_t)\\
		T_t &\propto \exp\left(\omega n_L(T_t) - \zeta \Delta(T_t)\right)\\
		\pi(M_t\mid T_t) & = \prod_{j=1}^{n_L(T_t)}\pi(\mu_j\mid T_t).
	\end{split}
\end{align}
where $\pi$ denotes the prior density.


\section{Conditional Copula Modelling}\label{sec:cond:cop}

Our main goal in this paper is to model the dependence structure of a conditional copula using regression trees. However, the conditional copula parameter $\theta(x)$ may have a specific range. Therefore, we use a suitable link function $h$ that maps a sum of trees model to the range of $\theta(x)$ in the following way:

\begin{equation*}
	\theta(x_i) \coloneqq h\left(\sum_{t=1}^m g(x_i, T_t, M_t)\right);\qquad 1\le i\le n.
\end{equation*}
Now, let $c\left(u_1,u_2\mid \theta(x)\right)$ denote the conditional copula density function. Then employing the loss-based prior for tree introduced in \cref{eq:L-BART}, we can define the following hierarchical model 
\begin{align}\label{eq:bayes:hier}
	\begin{split}
		u_{1i},u_{2i} \mid \theta(x_i) & \sim c\left(u_{1i},u_{2i}\mid h\left(\sum_{t=1}^m g(x_i, T_t, M_t)\right)\right)\\
		T_t &\propto \exp\left(\omega n_L(T_t) - \zeta \Delta(T_t)\right)\\
		\pi(M_t\mid T_t) &\propto \prod_{j=1}^{n_L(T_t)}\pi(\mu_j\mid T_t); \qquad t = 1,2,\cdots m.
	\end{split}
\end{align}

\subsection{Choice of link functions} The choice of link function to relate the tree structure with the conditional copula parameter is dependent on the family of copula. Use of a link function is not unusual in the context of conditional copula modelling. \citet{ABEGAZ201243,valle_cond_cop} used such functions for calibrating the conditional copula parameter. This way the sum of trees remains flexible and it can take any value in $\mathbb{R}$. Therefore, we consider link functions that can map from $\mathbb{R}$ to the range of $\theta$ of a specific copula family that we provide in \cref{tab:cop:link} for 5 different families.

\subsection{Choice of priors on $\mu_j$}
Earlier adaptions of BART models \citep{chipman2010BART,Sparapani_BART,Murray03042021} suggested a conjugate prior for the terminal node values $\mu_j$, this allows us to compute the marginal likelihood easily. In our case, we simply consider the default $\mathcal{N}(0,\sigma_{t}^2)$ on $\mu_j$ as suggested by \citet{chipman2010BART,Linero02012025} where $\sigma_{t}^2$ is variance specific to the $t$-th tree. For $\sigma_{t}^2$ we use a flat inverse-gamma hyperprior.

\subsection{Choice of $m$} The choice of the total number of trees remains an open problem. \citet{chipman2010BART} considered a default of 500 trees whereas, \citet{Linero02012025} suggested 200 trees for analysis. In our case, we follow a similar approach to that of \citet{serafini2024lossbasedpriortreetopologies}. We start with 5 trees and increase it by 5 to monitor the change in performance for case studies. 

\section{RJ-MCMC for Parameter Estimation}\label{sec:rjmcmc}

In this section we provide the MCMC algorithm for sampling from the posterior. In general, for BART models with a conjugate prior, a Metropolis Hastings type algorithm is used. In our case, we need to sample terminal node values along side a new tree structure in each MCMC iteration. So we will employ a reversible jump MCMC type algorithm first proposed by \citet{green_RJMCMC}. Since, RJ-MCMC type algorithms are known for its slow mixing properties, we propose an adaptive alternative that improves the mixing speed, without needing to spend much time on selecting the optimal parameter values of the proposal distribution.

\subsection{Backfitting algorithm}

The major building block of sum of tree models is the backfitting algorithm. We modify this backfitting algorithm to meet our needs, which we will discuss below. First, let $(U_1,U_2)$ denote the pseudo observations $\{(u_{1i},u_{2i}):1\le i \le n\}$ and $X$ denote the covariates. Then the posterior obtained from the hierarchical model described in \cref{eq:bayes:hier} is proportional to the following:
\begin{align*}
	\begin{split}
		\prod_{i=1}^{n}c\left(u_{1i},u_{2i}\mid h\left(\sum_{t=1}^m g(x_i, T_t, M_t)\right)\right)\prod_{t=1}^{m}\pi(T_t)\prod_{t=1}^{m}\left(\prod_{j=1}^{n_L(T_t)}\pi(\mu_j\mid T_t)\right)\prod_{t=1}^{m}\pi(\sigma_{t}).
	\end{split}
\end{align*}
To sample from such posterior formulations, \citet{chipman2010BART} suggested a Bayesian backfitting algorithm where we sample the $k$-th pair $(T_k,M_k)$ conditional on the other $m-1$ pairs of $(T_t,M_t)$. To be more precise, let 
\begin{equation*}
	(T_{-k},M_{-k})\coloneq \left\{(T_1,M_1),\cdots,(T_{k-1},M_{k-1}),(T_{k+1},M_{k+1}),\cdots(T_m,M_m)\right\},
\end{equation*}
then in each MCMC iteration, we sample $(T_k, M_k)$ from the following conditional distribution: 
\begin{equation*}
	(T_k,M_k)\mid T_{-k},M_{-k}, \sigma^2_{k}, U_1, U_2, X
\end{equation*}
followed by a sample of the variance term $\sigma_{k}$ from the following conditional distribution
\begin{equation*}
	\sigma^2_{k} \mid (T_k,M_k),U_1,U_2,X.
\end{equation*}
So, in order to implement the backfitting algorithm, we define 
\begin{equation*}
	R_{ik} = \sum_{t\not=k}g(x_i, T_t, M_t)\quad\text{and}\quad R_{\cdot k}\coloneqq(R_{1k},R_{2k},\cdots,R_{nk}).
\end{equation*}
Here $R_{ik}$ is similar to the notion of the residual discussed in \citet{chipman2010BART, serafini2024lossbasedpriortreetopologies}. However, unlike their problems of interest we are not fitting directly against the outcome data. Instead we will add these $R_{ik}$s with the terminal node values of the $k$-th regression tree, which give us the following conditional posterior
\begin{align}\label{eq:post:res}
	\begin{split}
		\pi(T_k,M_k \mid R_{\cdot k}, \sigma^2_{k}, U_1,U_2, X) &\propto \pi(T_k)\prod_{i=1}^{n}c\left(u_{1i},u_{2i}\mid h\left(R_{ik}+g(x_i, T_k, M_k)\right)\right)\prod_{j=1}^{n_L(T_k)}\pi(\mu_j\mid T_k)
	\end{split}
\end{align}
For the standard BART model, the use of a conjugate prior allows us to marginalise the likelihood with respect to $\mu\coloneqq\left(\mu_1,\mu_2,\cdots,\mu_{n_L(T_k)}\right)$ such that:
\begin{equation*}
	\mathcal{L}(U_1,U_2\mid X, T_k, \sigma^2_{k})\propto \bigint_{\mu}\left(\prod_{i=1}^{n}c\left(u_{1i},u_{2i}\mid h\left(R_{ik}+g(x_i, T_k, M_k)\right)\right)\prod_{j=1}^{n_L(T_k)}\pi(\mu_j\mid T_k)\right)d\mu.
\end{equation*}
Unfortunately, we do not have the conjugacy property in our case, and we use a reversible jump MCMC algorithm \citep{green_RJMCMC} to sample from the posterior given by \cref{eq:post:res}. We follow, a similar setup to that of \citet{Linero02012025} to build our algorithm. However, for tree space exploration, we consider the original tree steps suggested by \citep{chipman98BCART}.

\subsection{Proposal for RJ-MCMC}

Reversible jump MCMC is a common strategy for trans-dimensional cases where we need to grow or reduce the dimension of our model. This allows us to work around the issue of not having a conjugate prior for the terminal node values and we can sample $(T_k,M_k)$ efficiently in each iteration. For that, we consider a proposal function to generate a new pair $\left(T_k^\ast, M_k^\ast\right)$ at the $(\eta+1)$-th iteration given by:
\begin{align}\label{eq:prop}
	q\left(T_k^{\eta},M_k^{\eta};T_k^\ast, M_k^\ast\right) = q\left(T_k^{\eta};T_k^\ast\right) q_{\left(T_k^{\eta};T_k^\ast\right)}\left(M_k^{\eta};M_k^\ast\right).
\end{align}
Here, $q\left(T_k^{\eta};T_k^\ast\right)$ denotes the tree proposal as described by \citet{chipman98BCART}. They suggested four different tree steps given by:
\begin{itemize}
	\item \textsc{grow}: Randomly choose a terminal node and split it into two terminal nodes
	\item \textsc{prune}: Randomly choose a parent of terminal nodes and turn into a terminal node
	\item \textsc{change}: Randomly choose an internal node and assign a new splitting rule
	\item \textsc{swap}: Randomly choose a parent-child pair of internal node and swap their splitting rules
\end{itemize}
For a detailed discussion on the splitting rule, we suggest the readers to look into \citet{chipman98BCART,serafini2024lossbasedpriortreetopologies}. 

The generation of new terminal node values rely on the proposed tree structure that we represent with $q_{\left(T_k^{\eta};T_k^\ast\right)}\left(M_k^{\eta};M_k^\ast\right)$. Clearly, we only need a proposal for \textsc{grow} and \textsc{prune} steps to get new candidates, as the dimension does not change for the other two tree steps. Therefore, the proposal for terminal node values is given by:
\begin{itemize}
	\item For \textsc{grow} we consider the $j$-th leaf to be grown to $j_l$ and$j_r$ so,
	$q_{\left(T_k^{\eta};T_k^\ast\right)}\left(M_k^{\eta};M_k^\ast\right)$ = $\pi_{prop}(\mu_{j_l})\times\pi_{prop}(\mu_{j_r})$
	\item For \textsc{prune} we consider the $j_l$th and $j_r$th leaves to be pruned then $q_{\left(T_k^{\eta};T_k^\ast\right)}\left(M_k^{\eta};M_k^\ast\right)$ = $\pi_{prop}(\mu_j)$
\end{itemize}
where $\pi_{prop}$ denotes the proposal density function.

For the choice of proposal we consider a normal distribution with mean being the average of terminal node values at the $\eta$-th iteration and variance $\sigma_{\text{prop}}$. We notice that fixing $\sigma_{\text{prop}}$ is rather difficult. While monitoring the likelihood gives us some idea but, due to slow mixing nature of RJ-MCMC, convergence may take a longer iterations. So, we suggest an adaptive variance for the proposal. Our approach is motivated from the seminal work of \citet{haario_AMH} where they suggest the use of MCMC samples to update the co-variance. They also provided a simple updating formula that reduced the computation cost of the covariance matrix. However, in our case we need to adapt the method to facilitate the \textsc{grow} and \textsc{prune} moves. So we propose an adaptive covariance based on the partition of the predictor space, which we discuss below. First, we let our RJ-MCMC run for $\eta_0$ iterations without any adaption. We collect the values at observations with respect to each tree and each iteration defined by:
\begin{equation*}
	V_{ik}^{\eta} = g\left(x_i,T_k^{\eta},M_k^{\eta}\right)\quad 1\le i \le n; 1\le \eta \le \eta_0.
\end{equation*}
Now, let $V_{.k}^{\eta}\coloneq (V_{1k}^{\eta}, \cdots, V_{nk}^{\eta})$ denote the vector of values at observation for the $k$-th tree at $\eta$-th iteration. We will use these vectors of samples to compute the covariance matrix for all $\eta>\eta_0$.
\begin{equation*}
	C_k^{\eta} \coloneqq Cov\left(V_{\cdot k}^{1},V_{\cdot k}^{2},\cdots,V_{\cdot k}^{\eta-1}\right) + \epsilon\mathbf{I}_n \quad \text{for } \epsilon>0; \eta>\eta_0,
\end{equation*}
where $\mathbf{I}_n$ is the identity matrix of order $n$. This additional term $\epsilon\mathbf{I}_n$ ensures that the matrix is positive definite. Once we have this covariance matrix for the $t$-th tree, we can use this to calculate the proposal variance at a new terminal node value.

Let $\mathbf{\Omega}^{k}\coloneqq \left\{\Omega_j^{k}\right\}_{j=1}^{n_L(T_k)}$ denote the partition of the observations created by the $k$-th tree. Then for any $\Omega_{j}^{k}\in \mathbf{\Omega}^{k}$, we can define a set of indices $\mathcal{I}_{j}^{k}\coloneqq \left\{i:x_i\in \Omega_{j}^{k}\right\}$ that we will use for calculating the proposal variance at the $j$-th terminal node of the $k$-th tree in the following way:
\begin{equation}\label{eq:var:adapt}
	\sigma_{\text{prop};j}^2 \coloneqq \frac{2.4^2}{\left(\#\left\{\mathcal{I}_{j}^{k}\right\}\right)^3}
	\sum_{c\in\mathcal{I}_{j}^{k}}\sum_{d\in\mathcal{I}_{j}^{k}}\left[C_k^{\eta}\right]_{cd},
\end{equation}
where $\#(.)$ denotes the cardinality of a set. Lastly, we update $C_k^{\eta+1}$ using the iterative formula discussed by \citet{haario_AMH} given by:
\begin{equation}\label{eq:iter:var}
	C_k^{\eta+1} \coloneqq \frac{\eta-1}{\eta} C_k^{\eta} + \frac{1}{\eta}\left(\eta \left(\overline{V}_{\cdot k}^{\eta-1}\right)\left(\overline{V}_{\cdot k}^{\eta-1}\right)^T - (\eta+1)\left(\overline{V}_{\cdot k}^{\eta}\right)\left(\overline{V}_{\cdot k}^{\eta}\right)^T + \left({V}_{\cdot k}^{\eta}\right)\left({V}_{\cdot k}^{\eta}\right)^T + \epsilon\mathbf{I}_n\right).
\end{equation}
The formulation in \cref{eq:var:adapt} ensures that the proposal variance represents the variance of the mean value at observations at each terminal node, which is equivalent to using a marginal likelihood in traditional BART models. We summarise this algorithm in \cref{alg:ada:prop}.


\begin{algorithm}
	\caption{Computation of adaptive proposal}\label{alg:ada:prop}
	\begin{algorithmic}[1]
		\State Perform $\eta_0$ iterations with a fixed variance.
		
		\For{$k =1, \cdots, m$}
		\State Collect MCMC samples for initial $\eta_0$ iterations of the $k$-th tree:
		\begin{equation*}
			V_{ik}^{\eta} = g\left(x_i,T_k^{\eta},M_k^{\eta}\right)\quad 1\le i \le n; 1\le \eta \le \eta_0.
		\end{equation*}
		
		\State Calculate sample covariance matrix :
		\begin{equation*}
			C_k^{\eta} \coloneqq Cov\left(V_{\cdot k}^{1},V_{\cdot k}^{2},\cdots,V_{\cdot k}^{\eta-1}\right) + \epsilon\mathbf{I}_n \quad \text{for } \epsilon>0; \eta>\eta_0.
		\end{equation*}
		
		\State Calculate the proposal variance of the $j$-th terminal node value:
		\begin{equation*}
			\sigma_{\text{prop};j}^2 \coloneqq \frac{2.4^2}{\left(\#\left\{\mathcal{I}_{j}^{k}\right\}\right)^3}
			\sum_{c\in\mathcal{I}_{j}^{k}}\sum_{d\in\mathcal{I}_{j}^{k}}\left[C_k^{\eta}\right]_{cd};\qquad\mathcal{I}_{j}^{k}\coloneqq \left\{i:x_i\in \Omega_{j}^{k}\right\};\qquad 1\le j\le n_L(T_k).
		\end{equation*}
		
		\EndFor
		
		\State Update sample covariance using iterative formula using \cref{eq:iter:var}.
	\end{algorithmic}
\end{algorithm}

Once we have the proposals for both trees and the terminal node values, we define the acceptance probability in the following way:
\begin{equation}\label{eq:acc:prob}
	\alpha\left(T_k^{\eta},M_k^{\eta};T_k^\ast, M_k^\ast\right)
	= \min\left\{1,\frac{\mathcal{L}(U_1,U_2\mid T_k^\ast,M_k^\ast)\pi(T_k^\ast,M_k^\ast)q\left(T_k^\ast, M_k^\ast;T_k^{\eta},M_k^{\eta}\right)}
	{\mathcal{L}(U_1,U_2\mid T_k^{\eta},M_k^{\eta})\pi(T_k^{\eta},M_k^{\eta}) q\left(T_k^{\eta},M_k^{\eta};T_k^\ast, M_k^\ast\right)}\right\}.
\end{equation}
where 
\begin{equation*}
	\mathcal{L}(U_1,U_2\mid T_k, M_{k})\coloneqq \prod_{i=1}^{n}c\left(u_{1i},u_{2i}\mid h\left(R_{ik}+g(x_i, T_k, M_k)\right)\right).
\end{equation*}


Lastly, after sampling a new $(T_k^{\eta+1},M_k^{\eta+1})$, we update the values of $M_k^{\eta+1}$ using an MH step followed by a Metropolis within Gibbs to update $\sigma_{k}^2$ conditional on $(T_k,M_k)$. We summarise one iteration of the RJ-MCMC algorithm in \cref{alg:MCMC}.

\begin{algorithm}[h]
	\caption{One iteration of RJ-MCMC for copula BART}\label{alg:MCMC}
	\begin{algorithmic}[1]
		\State The $\eta$-th iteration gives us $(T_k^{\eta},M_k^{\eta})$.
		\State Set $\theta(x_i) \leftarrow h\left(\sum_{t=1}^{m} g(x_i, T_t^{\eta},M_t^{\eta})\right)$ for $i = 1, \ldots, n$.
		\For{$k = 1, \ldots, m$}
		\State Set $R_{ik} \leftarrow \sum_{t\not=k}g(x_i, T_t^{\eta},M_t^{\eta})$ for $i = 1, \ldots, n$.
		
		\State Sample $(T_k^\ast, M_k^\ast)$ using $q\left(T_k^\ast, M_k^\ast \mid T_k^{\eta},M_k^{\eta}\right)$ in \cref{eq:prop} by randomly choosing between the \textsc{grow}, \textsc{prune}, \textsc{change} and \textsc{swap}.
		
		\State Compute the acceptance probability using $\alpha\left(T_k^{\eta},M_k^{\eta};T_k^\ast, M_k^\ast\right)$ in \cref{eq:acc:prob}.
		
		\State Set $\left(T_k^{\eta+1}, M_k^{\eta+1}\right)=\left(T_k^\ast, M_k^\ast\right)$ with probability $\alpha\left(T_k^{\eta},M_k^{\eta};T_k^\ast, M_k^\ast\right)$ or $\left(T_k^{\eta+1}, M_k^{\eta+1}\right)=\left(T_k^{\eta},M_k^{\eta}\right)$.
		
		\State Set $\theta(x_i) \leftarrow h\left(R_{ik} + g\left(x_i, T_k^{\eta+1}, M_k^{\eta+1}\right)\right)$ for $i = 1, \ldots, n$
		
		\State Update $M_k^{\eta+1}$ using an MH step targeting the full conditional.
		
		\State Sample $\sigma_{k}^{2;(\eta+1)}$ from 
		\begin{equation*}
			\text{InvGamma}\left(a+\frac{n_L\left(T_k^{\eta+1}\right)}{2} , b + \frac{\sum_{j=1}^{n_L\left(T_k^{\eta+1}\right)}\mu_j^2}{2}\right)
		\end{equation*}
		\EndFor
	\end{algorithmic}
\end{algorithm}

\section{Simulation Studies}\label{sec:sim}

In this section, we present our analyses with two simulated datasets with a known data generating process. The first dataset is constructed to understand the efficiency of our method in recovering the true model. To be more precise, we use the first dataset to check whether we can avoid overfitting and if our method is able to determine the true tree structure, present in the data generation process. The second dataset is simulated using a more complicated function to check the efficiency of our method in estimating the dependence structure for general functions.

For both, we investigate the performance of the proposed method for five different copula families: Gaussian, Students-t, Clayton, Gumbel and Frank. As we discussed in \cref{sec:cond:cop}, to model the conditional copula parameter, we consider different link functions which we summarise in \cref{tab:cop:link}. This way, we ensure that the terminal node values can take any value in $\mathbb{R}$ and the link function maps it to the range of the conditional copula parameter. Similar approach was taken by \citet{ABEGAZ201243,valle_cond_cop} for calibrating the conditional copula parameter.

\begin{table}[H]
	\centering
	\begin{tabular}{l|c|c}
		Family & Support & Link function\\
		\midrule
		\thead{Gaussian \& \\
		Student-t} & $\rho \in (-1,1)$ & $h(x)=\frac{\exp(x)-1}{\exp(x)+1}$\\
		Clayton & $\theta \in (0,\infty)$ & $h(x)=\exp(x)$\\
		Gumbel & $\theta\in [1,\infty)$ & $h(x)=\exp(x)+1$\\
		Frank & $\theta\in \mathbb{R}\neg \{0\}$ & $h(x)=x$\\
		\end{tabular}
	\caption{List of copula families used for our analyses followed by the range of the conditional copula parameter and the corresponding link function.}
	\label{tab:cop:link}
\end{table}

For our analyses, we simulate 200 observations where the $x_i$-th predictor is sampled from a uniform distribution $U(0,1)$ for $1\le i\le 200$. The two sets of the conditional Kendall's taus are defined using the following equations:
\begin{equation}\label{eq:tree:tau}
	\tau_1(x_i) = \begin{cases}
		0.3 & x_i \le 0.33\\
		0.8 & 0.33 < x_i \le 0.66\\
		0.3 & 0.66 < x_i
	\end{cases}
\end{equation}
and
\begin{equation}\label{eq:sin:tau}
	\tau_2(x_i) = 0.2\sin(2\pi x_i) + 0.5.
\end{equation}
We use these $\tau_j(x_i)$'s ($j=1,2; 1\le i\le 200$) to simulate copula data using the relevant functions from \texttt{VineCopula} package in \texttt{R}. Additionally, we get 100 replicates of each copula dataset to monitor the posterior coverage of our proposed method.

For modelling of the dependence structure, we consider the default loss-based prior ($\omega = 1.62$ and $\zeta = 0.62$) as suggested by \citet{serafini2024lossbasedpriortreetopologies}. For the hyperior prior on $\sigma_t^2$, we consider InvGamma(1,2). We then run 3000 MCMC iterations with 4 chains for all 100 replicates of each dataset. We employ both types of our proposed RJ-MCMC algorithm; with and without adaption. For the adaptive version of the algorithm, we use the first 500 iterations for adaption. For convenience, from now on, we will use C-BART for our RJ-MCMC algorithm without adaption and A-C-BART for the adaptive version. For posterior estimates, we discard the first 1500 iterations as burnin phase.

One specific goal of our analyses is to understand the efficiency of our method in capturing the true structure, we consider the posterior expectations averaged over 100 replicates. Additionally, we are interested in the predictive performance of our approach and we consider RMSE, 95\% credible interval length and 95\% credible interval coverage. 

Let, $n_R, n_C, n$ denote the number of replicates, number of chains and number of observations respectively; and $\overline{\theta}_{ijk}, {\theta}^{(2.5)}_{ijk}, {\theta}^{(97.5)}_{ijk}$ denote the posterior mean, 2.5th quantile and 97.5th quantile of the conditional copula parameter at the $i$-th observation in the $j$-th MCMC chain of the $k$-th replicate. Most importantly, let $\theta_i$ denote the true value of the conditional copula parameter at the $i$-th observation. Then RMSE, 95\% credible interval length (CI-length) and 95\% credible interval coverage (CI-cov) is defined as follows

\begin{equation*}
	\text{RMSE} = \frac{1}{n_R}\sum_{k=1}^{n_R}\left(\frac{1}{n_C}\frac{1}{n}\sum_{j=1}^{n_C}\sum_{i=1}^{n}\left(\theta_i - \overline{\theta}_{ijk}\right)^2\right),
\end{equation*}

\begin{equation*}
	\text{CI-length} = \frac{1}{n_R}\frac{1}{n_C}\frac{1}{n}\sum_{k=1}^{n_R}\sum_{j=1}^{n_C}\sum_{i=1}^{n}\left({\theta}^{(97.5)}_{ijk}-{\theta}^{(2.5)}_{ijk}\right),
\end{equation*}
and
\begin{equation*}
	\text{CI-cov} = \frac{1}{n_R}\frac{1}{n_C}\frac{1}{n}\sum_{k=1}^{n_R}\sum_{j=1}^{n_C}\sum_{i=1}^{n}\mathbb{I}\left({\theta}^{(97.5)}_{ijk}\le\theta_i\le {\theta}^{(2.5)}_{ijk}\right)
\end{equation*}
where $\mathbb{I}(.)$ is the indicator function.

\subsection{Example with true tree structure} 

We use the first case ($\tau_1(x)$) to monitor the efficiency of our proposed approach in capturing the tree structure. So, for the analysis we consider only 1 tree. This way, we check how close our posterior estimates are to the true number of terminal nodes ($=3$) and true depth ($=2$). For the Gaussian, Student-t, Clayton and Gumbel copula datasets, we set our initial proposal variance to be equal to $0.2$, whereas for the Frank copula datasets, we set our proposal variance to be equal to $1$. The main reason behind this is the type of link function we chose for these datasets. Since for the frank copula datasets the link function is identity therefore it takes much longer iterations to reach the true likelihood region. For sanity, we also perform a complete analyses with proposal variance being equal to 0.2, which we provide in the \cref{app:results}.

To check the efficiency of our approach in recovering the true model, we look into the following quantities into posterior expectations of the number of terminal nodes and depth averaged over 100 replicates along with their respective standard deviations over 100 replicates. We denote these quantities by `Mean ($\hat{n}_L$)', `SD ($\hat{n}_L$)', `Mean ($\hat{D}$)' and `SD ($\hat{D}$)'.

We present the efficiency  \cref{tab:eff:ex1}. We notice that for both C-BART and A-C-BART our posterior estimates of the number of terminal nodes and depths are close to their true value. The slight deviation can be attributed to the tree exploration steps where the model tends grow leaves in tree MCMC steps. We also present the traceplots of the number of terminal nodes in \cref{fig:trace:nterm:ex1} and the depth in \cref{fig:trace:depth:ex1} for the first replicate corresponding to each copula dataset. We notice that the tree exploration stabilises after about 1000 iterations with our proposed approaches. We present the likelihood plot after discarding 1500 burn in samples in \cref{fig:trace:like:ex1}. We see that our method is able to converge towards the true likelihood region for all the cases, shown by the black dashed line. We also present the averaged acceptance rate over 100 replicates along with the standard deviation. We notice that the acceptance rate lies within $[0.16, 0.18]$, which is a reasonable range for a transdimensional MCMC algorithm. A similar effect is also reported by \citet{Linero02012025} regarding the mixing of the likelihood. 

Recall, that for the Frank copula family, we fixed the initial proposal variance to be equal to 1, while the proposal variance for all the other families is equal to 0.2. We also perform an additional analysis, fixing the proposal variance as 0.2 for the Frank copula family. This particular case, gives us an insight on the benefit of the adaptive version of our algorithm. We notice that only one out of 4 chains converge to true likelihood region for C-BART after 7500 iterations, whereas all 4 chains converge to the true likelihood region for A-C-BART. We present the traceplots of the number of terminal nodes, depth and likelihood in \cref{fig:trace:frank}. We also notice that the posterior estimates for the number of terminal nodes and depth of the tree is closer to the true value for A-C-BART and the credible interval coverage is significantly better than C-BART. We present these tables in \cref{app:results}

\begin{table}[ht]
	\centering
	\begin{tabular}{l|cccccc}
		\multicolumn{1}{c|}{} &
		\multicolumn{6}{c}{C-BART} \\
		\midrule
		& Mean ($\hat{n}_L$) & SD ($\hat{n}_L$) & Mean ($\hat{D}$) & SD ($\hat{D}$) & Mean Acc. & SD Acc. \\ 
		\midrule
		Gaussian & 3.149 & 0.110 & 2.059 & 0.095 & 0.161 & 0.011 \\ 
		Student-t & 3.109 & 0.114 & 2.016 & 0.093 & 0.161 & 0.010 \\ 
		Clayton & 3.177 & 0.065 & 2.046 & 0.055 & 0.179 & 0.007 \\ 
		Gumbel & 3.143 & 0.044 & 2.033 & 0.036 & 0.171 & 0.010 \\ 
		Frank & 3.308 & 0.167 & 2.190 & 0.142 & 0.174 & 0.011 \\
		\midrule
		\multicolumn{1}{c|}{} &
		\multicolumn{6}{c}{A-C-BART} \\
		\midrule
		Gaussian & 3.141 & 0.107 & 2.049 & 0.093 & 0.161 & 0.011 \\ 
		Student-t & 3.084 & 0.150 & 1.995 & 0.134 & 0.160 & 0.011 \\ 
		Clayton & 3.199 & 0.093 & 2.065 & 0.079 & 0.179 & 0.008 \\ 
		Gumbel & 3.146 & 0.045 & 2.035 & 0.034 & 0.170 & 0.010 \\ 
		Frank & 3.251 & 0.174 & 2.141 & 0.150 & 0.172 & 0.010 \\
	\end{tabular}
	\caption{Efficiency of our proposed method for simulated dataset using a tree based conditional Kendall's tau (\cref{eq:tree:tau}). We present the average and standard deviation of the posterior expectation of the number of terminal nodes; the posterior expectation of the depth of the tree; and the acceptance rate in $[0,1]$ scale.}
	\label{tab:eff:ex1}
\end{table}

\begin{figure}
	\centering
	\includegraphics[width = 0.95\linewidth]{"case_1_nterm.pdf"}
	\caption{Trace plots of the number of terminal nodes obtained from our analyses with synthetic datasets generated using tree based Kendall's tau (\cref{eq:tree:tau}) for the first replicate. The plots are obtained by running 4 parallel chains each one with 3000 MCMC iterations. The left columns denote analyses with C-BART and the right column shows analyses with A-C-BART. The horizontal dashed black line represents the number of terminal nodes (3) of the data-generating tree.}
	\label{fig:trace:nterm:ex1}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width = 0.95\linewidth]{"case_1_depth.pdf"}
	\caption{Trace plots of the depth of the tree obtained from our analyses with synthetic datasets generated using tree based Kendall's tau (\cref{eq:tree:tau}) for the first replicate. The plots are obtained by running 4 parallel chains each one with 3000 MCMC iterations. The left columns denote analyses with C-BART and the right column shows analyses with A-C-BART. The horizontal dashed black line represents the depth (2) of the data-generating tree.}
	\label{fig:trace:depth:ex1}
\end{figure}


\begin{table}[ht]
	\centering
	\begin{tabular}{l|ccc|ccc}
		\multicolumn{1}{c|}{} &
		\multicolumn{3}{c|}{C-BART} &
		\multicolumn{3}{c}{A-C-BART} \\
		\midrule
		& RMSE & CI-length & CI-cov & RMSE & CI-length & CI-cov \\ 
		\midrule
		Gaussian & 0.075 & 0.218 & 0.932 & 0.076 & 0.219 & 0.921 \\ 
		Student-t & 0.093 & 0.277 & 0.930 & 0.098 & 0.274 & 0.908 \\ 
		Clayton & 0.067 & 0.192 & 0.925 & 0.067 & 0.192 & 0.923 \\ 
		Gumbel & 0.069 & 0.217 & 0.959 & 0.070 & 0.216 & 0.936 \\ 
		Frank & 0.083 & 0.251 & 0.930 & 0.083 & 0.254 & 0.925 
	\end{tabular}
	\caption{Prediction accuracy of our proposed method for simulated dataset using a tree based conditional Kendall's tau (\cref{eq:tree:tau}). We split our results in two general columns: left of which is for C-BART, whereas the right one is for A-C-BART. We then create subcolumns under each column to present root mean squared error (RMSE); 95\% credible interval length (CI-length); and 95\% credible interval coverage (CI-cov).}
	\label{tab:pred:ex1}
\end{table}

\begin{figure}
	\centering
	\includegraphics[width = 0.95\linewidth]{"case_1_likelihood.pdf"}
	\caption{Trace plots of log-likelihood obtained from our analyses with synthetic datasets generated using tree based Kendall's tau (\cref{eq:tree:tau}). The plots are obtained by running 4 parallel chains with 3000 MCMC iterations each. The left columns denote analyses with C-BART and the right column shows analyses with A-C-BART. The horizontal dashed black line represents the log-likelihood of the data-generating tree.}
	\label{fig:trace:like:ex1}
\end{figure}

%  79.78078 110.74562 115.01339 102.35259  86.50401

\subsection{Example with a general function}
We use the second case ($\tau_2(x)$) to check the prediction accuracy of our proposed approach in estimating the dependence structure that is highly non-linear with respect to the conditioning variable. For illustration purpose we use 5 trees to model the conditional copula. To illustrate the prediction accuracy, we used RMSE, CI length and CI coverage similar to our analyses with true tree structure. For posterior analysis we set the proposal variance to be equal to 0.2 for all copula families.

We present these results in \cref{tab:pred:ex2}, we notice that A-C-BART and C-BART are in good agreements in terms of prediction accuracy. We also present the prediction plots corresponding the first replicate of the copula datasets in \cref{fig:trace:pred:ex2}. In the figure, we represent the true values of Kendall's conditional tau with red points; posterior estimates of Kendall's tau with black line and credible interval with green lines. 


\begin{table}[ht]
	\centering
	\begin{tabular}{l|ccc|ccc}
		\multicolumn{1}{c|}{} &
		\multicolumn{3}{c|}{C-BART} &
		\multicolumn{3}{c}{A-C-BART} \\
		\midrule
		& RMSE & CI-length & CI-cov & RMSE & CI-length & CI-cov \\ 
		\midrule
		Gaussian & 0.074 & 0.318 & 0.961 & 0.070 & 0.321 & 0.968 \\ 
		Student-t & 0.082 & 0.377 & 0.978 & 0.082 & 0.381 & 0.981 \\ 
		Clayton & 0.073 & 0.304 & 0.960 & 0.070 & 0.306 & 0.966 \\ 
		Gumbel & 0.079 & 0.329 & 0.952 & 0.077 & 0.333 & 0.957 \\ 
		Frank & 0.070 & 0.278 & 0.926 & 0.068 & 0.284 & 0.940 \\  
	\end{tabular}
	\caption{Prediction accuracy of our proposed method for simulated dataset using a non-linear conditional Kendall's tau (\cref{eq:sin:tau}). We split our results in two general columns: left of which is for C-BART and the right one is for A-C-BART. We then create three subcolumns under each column to present root mean squared error (RMSE); 95\% credible interval length (CI-length); and 95\% credible interval coverage (CI-cov).}
	\label{tab:pred:ex2}
\end{table}

\begin{figure}
	\centering
	\includegraphics[width = 0.95\linewidth]{"case_2_pred.pdf"}
	\caption{Comparison of BART fit obtained from our analyses with synthetic datasets generated using a non-linear Kendall's tau (\cref{eq:sin:tau}). We fit the model using 5 trees. We run 4 chains in parallel with 3000 MCMC iterations. The left columns denote analyses with C-BART and the right column shows analyses with A-C-BART. The green line denote the 95\% credible interval; the black line denotes the posterior estimate of Kendall's tau; and the red points show the true values of conditional Kendall's tau defined in \cref{eq:sin:tau}.}
	\label{fig:trace:pred:ex2}
\end{figure}

Additionally, we present the likelihood traceplots in \cref{fig:trace:like:ex2}. We notice that both C-BART and A-C-BART is able to move towards the true likelihood region, suggesting the efficiency of our method in exploring the parameter space.

\begin{figure}
	\centering
	\includegraphics[width = 0.95\linewidth]{"case_2_likelihood.pdf"}
	\caption{Trace plots of log-likelihood obtained from our analyses with synthetic datasets generated using a non-linear Kendall's tau (\cref{eq:sin:tau}). The plots are obtained by running 4 parallel chains with 3000 MCMC iterations each. The left columns denote analyses with C-BART and the right column shows analyses with A-C-BART. The horizontal dashed black line represents the log-likelihood of the data-generating process.}
	\label{fig:trace:like:ex2}
\end{figure}

% 99.80067 96.13416 78.41696 77.33286 69.78601

\section{CIA world factbook data}\label{sec:cia}
Here we present two case studies with real dataset to discuss the applicability of our method. For that, we use the CIA world factbook data. The data is originally produced by CIA for policymakers in the USA and contains important insights on different countries. We are particularly interested in the `life expectancy' and the `literacy' of male and female population in different countries. For conditioning factor we consider the economy of these countries represented by the gross domestic product. 

For illustration purpose, we run 4 parallel chains of 30000 MCMC iterations with 5 and 10 trees for both C-BART and A-C-BART. For A-C-BART, we use the first 1000 iterations to calculate the proposal variance.

Lastly, for monitoring goodness-of-fit, we consider two different two-sample tests to understand the goodness-of-fit of the simulated copulas: Cramer test for multivariate data \citep{BARINGHAUS2004190}; and Fasano-Franceschini test \citep{fasano-franceschini}, which is a generalised version of Kolmogorov–Smirnov test. These tests are designed to check if two sets of samples belong to the same distribution or not. The null hypothesis is rejected if the $p$-value is below $0.05$. Since these are permutation based tests, we perform 100 repeated tests each involving 100 permutations.

\subsection{Life Expectancy}
Here, we analyse the dependence between the female life expectancy and male life expectancy against the GDP in log-scale. We use total 167 observations for our analyses. We see that the female life expectancy lies within $[56.1,89.5]$ with average being 76 years. For male population these numbers are slightly lower; the range being $[52.8, 84]$ and average being 71 years. We present the distribution of these observations against the log GDP in \cref{fig:data:dist:LE} along with the pseudo observations. We notice that the life expectancy has a very strong tail dependence and the Kendall's is equal to 0.81. 
 
\begin{figure}
	\centering
	\includegraphics[width = 0.95\linewidth]{"cia_LE_plots.pdf"}
	\caption{Scatterplot of male life expectancy against log-GDP (left); scatterplot of female life expectancy against log-GDP (middle); and pseudo observations with respect to log-GDP (right).}
	\label{fig:data:dist:LE}
\end{figure}

We notice that the pseudo observations has an elliptical shape. Therefore, to model the dependence structure we consider Gaussian and Student-t copula families. We notice that for both C-BART and A-C-BART the estimated conditional Kendall's taus are in good agreement for both families of copula. We present the conditional taus in \cref{fig:taus:LE}, where the red (blue) solid line shows expected conditional taus for the Gaussian (Student-t) copula and the dashed lines show the credible interval. We notice that countries with lower GDP shows a mild dependence between male and female life expectancy with value being close to 0.5. As the GDP increases, we notice that the dependence increases rapidly to around 0.9. Later on, for countries with medium log-GDP, the conditional Kendall's tau becomes stable around 0.8. However, for countries with extremely high GDP the dependence between male and female life expectancy becomes very high again. This can also be verified from the scatterplots in \cref{fig:data:dist:LE}.
 
\begin{figure}
	\centering
	\includegraphics[width = 0.95\linewidth]{"LE_vs_GDP_taus.pdf"}
	\caption{Estimated dependence between male life expectancy and female life expectancy conditional on scaled log-GDP. For modelling, we use 5 trees and run 4 parallel chains involving 30000 iterations. For posterior inference, we discard the first 5000 samples.}
	\label{fig:taus:LE}
\end{figure}
% 0.81

We present the traceplots of our analyses in \cref{fig:trace:like:real:LE}. We notice an interesting behaviour of A-C-BART for the Gaussian copula. We notice that one chain explores a higher likelihood regions for a few iterations, something we did not notice for the Student-t copula. 

\begin{figure}
	\centering
	\includegraphics[width = 0.95\linewidth]{"LE_vs_GDP_likelihood.pdf"}
	\caption{Trace plots of log-likelihood obtained from our analyses with life expectancy of male and female population. The plots are obtained by running 4 parallel chains each one with 30000 MCMC iterations and 5 trees. The left columns shows analyses with C-BART and the right column shows analyses with A-C-BART.}
	\label{fig:trace:like:real:LE}
\end{figure}

Additionally, we present the scatterplots and 3-d histograms of the simulated copulas in \cref{fig:pseudo:LE:woa} (C-BART) and \cref{fig:pseudo:LE:wa} (A-C-BART). From the scatter-plots, we can see that both C-BART and A-C-BART in agreement with each other though with A-C-BART the lower and upper tail dependences are captured better than that with C-BART. Unfortunately, there's no efficient ways to compute a similarity measure. So, we present the results of goodness-of-fit tests in \cref{tab:LE:p-val}. We notice that all the $p$-values are well above 0.05 suggesting that there is no significant evidence that they are from two different distributions. Specifically, for A-C-BART with Student-t copula, the $p$-values are well above 0.95, suggesting that Student-t copula is good choice for analysing the conditional dependence between the life expectancy of female and the life expectancy of male conditional on log-GDP. Moreover, we also present the goodness-of-fit test results for analyses with 10 trees. We notice no significant improvement in the test results. 

We present the figures for our analyses with 10 trees in \cref{app:cia}. We notice that with 10 trees, our model tends to explore higher likelihood regions more frequently and A-C-BART tends to converge earlier in a higher likelihood region. While it is interesting for showcasing the convergence of A-C-BART, it does not improve the goodness-of-fit significantly. Moreover, having more trees in the model also increases the overall variance in the estimation of conditional Kendall's tau, which we can see in \cref{fig:taus:LE10}.


\begin{figure}
	\centering
	\includegraphics[width = 0.95\linewidth]{"LE_vs_GDP_woa.pdf"}
	\caption{Scatter plots of pseudo observations and predicted copulas obtained for life expectancy of male and female population using C-BART with 5 trees.}
	\label{fig:pseudo:LE:woa}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width = 0.95\linewidth]{"LE_vs_GDP_wa.pdf"}
	\caption{Scatter plots of pseudo observations and predicted copulas obtained for life expectancy of male and female population using A-C-BART with 5 trees.}
	\label{fig:pseudo:LE:wa}
\end{figure}

\begin{table}[ht]
	\centering
	\begin{tabular}{l|ccc|ccc}
		\multicolumn{1}{c|}{} &
		\multicolumn{3}{c|}{Cramer test} &
		\multicolumn{3}{c}{FF test} \\
		\midrule
		  & mean & median & sd & mean & median & sd \\ 
		\midrule
		\multicolumn{7}{c}{5 trees} \\
		\midrule
		Gaussian (C-BART) & 0.64 & 0.63 & 0.05 & 0.53 & 0.53 & 0.05 \\ 
		Student-t (C-BART) & 0.82 & 0.82 & 0.04 & 0.64 & 0.64 & 0.05 \\ 
		Gaussian (A-C-BART) & 0.85 & 0.85 & 0.03 & 0.75 & 0.76 & 0.04 \\ 
		Student-t (A-C-BART) & 0.96 & 0.96 & 0.02 & 0.99 & 0.99 & 0.01 \\ 
		\midrule
		\multicolumn{7}{c}{10 trees} \\
		\midrule
		Gaussian (C-BART) & 0.64 & 0.63 & 0.05 & 0.56 & 0.56 & 0.05 \\ 
		Student-t (C-BART) & 0.83 & 0.82 & 0.04 & 0.74 & 0.74 & 0.04 \\ 
		Gaussian (A-C-BART) & 0.85 & 0.85 & 0.03 & 0.74 & 0.74 & 0.05 \\ 
		Student-t (A-C-BART) & 0.96 & 0.96 & 0.02 & 0.98 & 0.99 & 0.01 \\ 
	\end{tabular}
	\caption{Goodness of fit test of our proposed method for life expectancy conditional on log-GDP. We split our results in two general columns: left of which is for Cramer test and the right one is for Fasano-Franceschini test (FF test). We then create subcolumns under each column to present mean, median $p$-values calculated from 100 repetitions along with the standard deviation.}
	\label{tab:LE:p-val}
\end{table}




\subsection{Literacy}
In this section we discuss our analyses with the literacy rate of different countries. We provide the scatterplots and distribution of pseudo observations for literacy rate of male and female population in \cref{fig:data:dist:LT}. For female population, the literacy rate varies within $[18.2,100]$ and the average literacy rate is 84\%, whereas for the male population, the literacy rate lies in $[35.4,100]$ with average literacy being 89\%. Unlike our example with the life expectancy, the literacy rate has no clear trend against the log-GDP. However, Kendall's tau computed from the data is approximately 0.84, indicating a strong dependence between the literacy rate of Male and Female population. 
\begin{figure}
	\centering
	\includegraphics[width = 0.95\linewidth]{"cia_LT_plots.pdf"}
	\caption{Scatterplot of male literacy against log-GDP (left); scatterplot of female literacy against log-GDP (middle); and pseudo observations with respect to log-GDP (right).}
	\label{fig:data:dist:LT}
\end{figure}

Similar to our analyses with life expectancy, we notice that there is an overall agreement between the estimated conditional dependence by C-BART and A-C-BART. However, in this case, the families of copula tend to behave differently. While there is a strong similarity between the Gaussian and Student-t copula, the Gaussian copula exhibits an abrupt drop in the conditional dependence, which we present in \cref{fig:taus:LT}. Other than that, for both families, the literacy rate of the Male and the Female population have an almost constant dependence 
with respect to the log-GDP.  We also present the traceplots of log-likelihood in \cref{fig:trace:like:real:LT}. We notice that for both families, C-BART and A-C-BART are in synergy and explores the similar likelihood regions. 

We present the scatterplots and 3-d histograms of simulated copula in \cref{fig:pseudo:LT:woa} and \cref{fig:pseudo:LT:wa}; and the result of goodness-of-fit tests in \cref{tab:LT:p-val}. Like before, we notice that there is no significant evidence that the simulated copulas and the observed copulas belong to two different families. Moreover, similar to our analyses with the life expectancy copula, Student-t copula with A-C-BART performs particularly well in terms of goodness-of-fit. We also present our results with 10 trees and see no significant improvement. Moreover, from the likelihood traceplots given by \cref{fig:trace:like:real:LT10}, we notice that our analyses with 5 trees and 10 trees give us almost similar results in exploring different likelihood regions, unlike our case study with the life expectancy.

\begin{table}[ht]
	\centering
	\begin{tabular}{l|ccc|ccc}
		\multicolumn{1}{c|}{} &
		\multicolumn{3}{c|}{Cramer test} &
		\multicolumn{3}{c}{FF test} \\
		\midrule
		 & mean & median & sd & mean & median & sd \\ 
		\midrule
		\multicolumn{7}{c}{5 trees} \\
		\midrule
		Gaussian (C-BART) & 0.55 & 0.54 & 0.05 & 0.58 & 0.57 & 0.05 \\ 
		Student-t (C-BART) & 0.78 & 0.77 & 0.04 & 0.66 & 0.66 & 0.04 \\ 
		Gaussian (A-C-BART) & 0.85 & 0.85 & 0.03 & 0.66 & 0.66 & 0.05 \\ 
		Student-t (A-C-BART) & 0.98 & 0.98 & 0.01 & 0.95 & 0.95 & 0.02 \\ 
		\midrule
		\multicolumn{7}{c}{10 trees} \\
		\midrule
		Gaussian (C-BART) & 0.56 & 0.56 & 0.05 & 0.64 & 0.64 & 0.05 \\ 
		Student-t (C-BART) & 0.77 & 0.77 & 0.04 & 0.58 & 0.58 & 0.05 \\ 
		Gaussian (A-C-BART) & 0.86 & 0.86 & 0.03 & 0.76 & 0.76 & 0.04 \\ 
		Student-t (A-C-BART) & 0.98 & 0.98 & 0.01 & 0.96 & 0.96 & 0.02 \\ 
	\end{tabular}
	\caption{Goodness of fit test of our proposed method for literacy conditional on log-GDP. We split our results in two general columns: left of which is for Cramer test and the right one is for Fasano-Franceschini test (FF test). We then create subcolumns under each column to present mean, median $p$-values calculated from 100 repetitions along with the standard deviation.}
	\label{tab:LT:p-val}
\end{table}

\begin{figure}
	\centering
	\includegraphics[width = 0.95\linewidth]{"LT_vs_GDP_likelihood.pdf"}
	\caption{Trace plots of log-likelihood obtained from our analyses with literacy of male and female population. The plots are obtained by running 4 parallel chains each one with 30000 MCMC iterations and 5 trees. The left columns shows analyses with C-BART and the right column shows analyses with A-C-BART.}
	\label{fig:trace:like:real:LT}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width = 0.95\linewidth]{"LT_vs_GDP_taus.pdf"}
	\caption{Estimated dependence between male literacy and female literacy, conditional on scaled log-GDP. For modelling, we use 5 trees and run 4 parallel chains involving 30000 iterations. For posterior inference, we discard the first 5000 samples.}
	\label{fig:taus:LT}
\end{figure}
% 0.84
\begin{figure}
	\centering
	\includegraphics[width = 0.95\linewidth]{"LT_vs_GDP_woa.pdf"}
	\caption{Scatter plots of pseudo observations and predicted copulas obtained for literacy of male and female population using C-BART with 5 trees.}
	\label{fig:pseudo:LT:woa}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width = 0.95\linewidth]{"LT_vs_GDP_wa.pdf"}
	\caption{Scatter plots of pseudo observations and predicted copulas obtained for literacy of male and female population using A-C-BART with 5 trees.}
	\label{fig:pseudo:LT:wa}
\end{figure}




\section{Conclusion}\label{sec:conc}

In this paper, we propose a Bayesian semi-parametric approach for modelling conditional copulas where we use the loss-based BART prior proposed by \citet{serafini2024lossbasedpriortreetopologies} to estimate the conditional copula parameter. To sample from the posterior, we propose a novel adaptive RJ-MCMC routine that can efficiently update the proposal variance of the terminal node values avoiding any manual tuning. In this way, we achieve a more general sampling algorithm for BART based models where we do not require a conjugate prior for posterior inference or a smooth likelihood function for applying other approximation based algorithm. Even though our proposed method is catered towards copula modelling, the lack of restrictions in our model, makes it desirable for other areas of modelling where we can employ BART. Moreover, in our framework, we use a link function to estimate the conditional copula parameter. However, in some cases, one might be interested in estimating the conditional Kendall's tau directly. Due to the flexible nature of our model, one can easily adapt our model to meet such needs by changing the link functions on the sum of trees model accompanied by a transformed beta prior \citep{gokhale_prior_cor} for the terminal node values.

We perform extensive simulation studies to show the efficiency of our methods in recovering the true tree structure present within the data as well as its ability to approximate a complicated function efficiently. We notice that specifically for complicated functions, where we require multiple trees, the adaptive routine performs better in reducing the prediction error, suggesting applicability. Besides that, we also illustrate our models adaptive capability using a case, where our adaptive RJ-MCMC routine converges towards the true likelihood region even with a sub optimal choice for proposal variance suggesting its usefulness in statistical analysis of real datasets where assigning proposal variance can be difficult. 

We also present case studies with real dataset involving life expectancy and literacy rate of male and female population of different countries. For conditioning, we consider GDP of each country to see whether there is a dependence between male and female populations regarding aforementioned metrics. We notice that our proposed method gives us some interesting results and empirical `goodness of fit' tests suggest that our method is capable of estimating the dependence structure correctly. Moreover, some of these case studies also show the benefit of adaptation, as we notice that our adaptive routine converges to a high likelihood region faster than the non-adaptive counterpart.

While we get promising results with our method across simulation and real case studies, we still require an objective way to fix the number of trees in our BART based copula models. Currently, in our work we use the goodness-of-fit to see the improvement in models with respect to increasing number of trees. This is gives us an empirical idea whether to use more trees or not. However, this makes our analyses computationally expensive as we need to fit multiple models to decide on the number of trees. Therefore, in the future, our main goal will be to find a clever model selection routine to fix the number of trees. Moreover, in this article, we only focus on bivariate copulas with a single confounding variable. It will be an immediate objective to extend the approach for multivariate copulas in the presence of multiple confounding variables to monitor our methods efficiency. Last but not the least, as we stated earlier, our sampling routine requires very few restrictions on the type of data we are using, therefore, it will be interesting to see the performance of our proposed method for general class of problems involving BART, where other methods may fail. 

\appendix

\section{Description of RJ-MCMC}

\subsection{Markov transition kernel of the RJ-MCMC step}

Let $\mathcal{T}\times \mathcal{M}$ a the state space and $T_k,M_k \in \mathcal{T}\times \mathcal{M}$ be the current state of the $k$-th tree. Let $\mathcal{K}_{\gamma}$ denote the transition kernel of the RJ-MCMC algorithm with $\gamma$ adaption and $p_{\textsc{g}}$, $p_{\textsc{p}}$, $p_{\textsc{c}}$ and $p_{\textsc{s}}$ be the probabilities of selecting \textsc{grow}, \textsc{prune}, \textsc{change} and \textsc{swap} move respectively. Then we can decompose the kernel in the following way:
\begin{align*}
	\mathcal{K}_{\gamma}\left(T_k,M_k, \cdot\right) 
	& = p_{\textsc{g}}\mathcal{K}_{\gamma;\textsc{grow}}\left(T_k,M_k, \cdot\right) + 
	p_{\textsc{p}}\mathcal{K}_{\gamma;\textsc{prune}}\left(T_k,M_k, \cdot\right)\\
	&\qquad + 
	p_{\textsc{c}}\mathcal{K}_{\gamma;\textsc{change}}\left(T_k,M_k, \cdot\right)+ 
	p_{\textsc{s}}\mathcal{K}_{\gamma;\textsc{swap}}\left(T_k,M_k, \cdot\right).
\end{align*}
Here, for the \textsc{change} and \textsc{swap} moves, we do not need to propose any new terminal node values hence $\gamma$ adaption is not utilised and the notation is used for the sake of consistency. Each of this kernel is given by:
\begin{align*}
	&\mathcal{K}_{\gamma;\textsc{move}}\left(T_k,M_k, \cdot\right) = \alpha_{\gamma;\textsc{move}}\left(T_k,M_k;T_k^\ast, M_k^\ast\right) q_{\gamma;\textsc{move}}\left(T_k,M_k;T_k^\ast, M_k^\ast\right)d(T_k^\ast, M_k^\ast) \\
	&+ \left(1 - \int_{\mathcal{T}\times\mathcal{M}} \alpha_{\gamma;\textsc{move}}\left(T_k,M_k;T_k^\ast, M_k^\ast\right) q_{\gamma;\textsc{move}}\left(T_k,M_k;T_k^\ast, M_k^\ast\right)d(T_k^\ast, M_k^\ast)\right) \delta_{(T_k,M_k)}(d(T_k^\ast, M_k^\ast))
\end{align*}
for $\textsc{move}\in \left\{\textsc{grow}, \textsc{prune}, \textsc{change}, \textsc{swap}\right\}$ with $\gamma$ adaption and $\delta_x(dy)$ as the Dirac measure at $x$.

\subsubsection{Proposal for each moves}

Before describing the detailed proposal for each moves corresponding to the pair $(T_k,M_k)$, we first define the following:
\begin{itemize}
	\item $\mathcal{TN}(T_k)=$ set of terminal nodes
	\item $\mathcal{PN}(T_k)=$ set of prunable nodes
	\item $\mathcal{IN}(T_k)=$ set of internal nodes
	\item $\mathcal{PCN}(T_k)=$ set of internal parent-child pairs
	\item $\pi_{\textsc{rule}}(l\mid c)=$ probability of splitting $l$-th terminal node at cutpoint $c$
\end{itemize}
Lastly, let $AR_{\gamma;\textsc{move}}\left(T_k,M_k;T_k^\ast, M_k^\ast\right)$ denote the acceptance ratio of moving from $(T_k,M_k)$ to $(T_k^\ast,M_k^\ast)$, such that
\begin{equation*}
	\alpha_{\gamma;\textsc{move}}\left(T_k,M_k;T_k^\ast, M_k^\ast\right) = \min\left\{1,AR_{\gamma;\textsc{move}}\left(T_k,M_k;T_k^\ast, M_k^\ast\right)\right\},
\end{equation*}
where $\textsc{move}\in \left\{\textsc{grow}, \textsc{prune}, \textsc{change}, \textsc{swap}\right\}$. Then the proposals and acceptance ratios are given in the following manners.

\paragraph{\textsc{grow}/\textsc{death} move:}
For growing a tree $T^{\ast}_k$ from $T_k$, we first randomly sample a terminal node $j$ from the set of all terminal nodes. Followed by a cut point for the splitting rule of the terminal node. Lastly, we sample terminal node values for the new left and right terminal nodes. Then we have the following:
\begin{equation*}
	q\left(T_k,M_k;T_k^{\ast}, M_k^{\ast}\right) 
	= \frac{1}{\#\{\mathcal{TN}(T_k)\}} \pi_{\textsc{rule}}(c\mid j)\mathcal{N}\left(\mu_{jr}^{*};\mu_{jr},\gamma_{jr}\right)\mathcal{N}\left(\mu_{jl}^{*};\mu_{jl},\gamma_{jl}\right),
\end{equation*} 
where $\mathcal{N}(\mu^{\ast};\mu,\gamma)$ is the probability of getting $\mu^{\ast}$ from a normal distribution with mean $\mu$ and variance $\gamma$. To go back from $T^{\ast}_k$ to $T_k$, we prune the $jl$-th terminal node and $jr$-th terminal node. For that, we calculate the probability of choosing the $j$-th node that becomes an internal node now. Followed by the proposal for terminal node value. Then,
\begin{equation*}
	q\left(T_k^{\ast}, M_k^{\ast};T_k,M_k\right) 
	= \frac{1}{\#\{\mathcal{PN}(T_k^{\ast})\}} \mathcal{N}\left(\mu_j;\mu_{j}^{*},\gamma_j^{*}\right).
\end{equation*} 
Therefore, the proposal ratio of the \textsc{growth} move is given by:
\begin{equation*}
	\frac{q\left(T_k^{\ast}, M_k^{\ast};T_k,M_k\right)}{q\left(T_k,M_k;T_k^{\ast}, M_k^{\ast}\right) } 
	= \frac{\frac{1}{\#\{\mathcal{PN}(T_k^{\ast})\}} \mathcal{N}\left(\mu_j;\mu_{j}^{*},\gamma_j^{*}\right)}{\frac{1}{\#\{\mathcal{TN}(T_k)\}} \pi_{\textsc{rule}}(c\mid j)\mathcal{N}\left(\mu_{jr}^{*};\mu_{jr},\gamma_{jr}\right)\mathcal{N}\left(\mu_{jl}^{*};\mu_{jl},\gamma_{jl}\right)}.
\end{equation*}
Similarly, we get the prior ratio for a \textsc{growth} move. Note that when a tree is grown at the $j$-th terminal node, we need additional splitting rule at that node. Therefore the ratio is given by:
\begin{equation*}
	\frac{\pi(T_k^{\ast},M_k^{\ast})\pi_{\textsc{rule}}(c\mid j)}{\pi(T_k,M_k)}.
\end{equation*}
Finally, combining with the likelihood we get
\begin{align*}
	&AR_{\gamma;\textsc{grow}}\left(T_k,M_k;T_k^\ast, M_k^\ast\right)\\
	&= \frac{
		\frac{\pi(T_k^\ast,M_k^\ast)}{\#\{\mathcal{PN}(T_k^{\ast})\}} 
		\prod_{i\in \mathcal{I}_{jr}^k}c\left(u_{1i},u_{2i}\mid h\left(R_{ik}+\mu^\ast_{jr}\right)\right)
		\prod_{i\in \mathcal{I}_{jl}^k}c\left(u_{1i},u_{2i}\mid h\left(R_{ik}+\mu^\ast_{jl}\right)\right)
		\mathcal{N}\left(\mu_j;\mu_{j}^{\ast},\gamma_j^{\ast}\right)}
	{\frac{\pi(T_k,M_k)}{\#\{\mathcal{TN}(T_k)\}} 
		\prod_{i\in \mathcal{I}_{j}^k}c\left(u_{1i},u_{2i}\mid h\left(R_{ik}+\mu_j\right)\right) 
		\mathcal{N}\left(\mu_{jr}^{\ast};\mu_{jr},\gamma_{jr}\right)\mathcal{N}\left(\mu_{jl}^{\ast};\mu_{jl},\gamma_{jl}\right)}.
\end{align*}
Since \textsc{prune} is the reverse process of the \textsc{grow} move, we have
\begin{align*}
	&AR_{\gamma;\textsc{prune}}\left(T_k,M_k;T_k^\ast, M_k^\ast\right)\\
	&= \frac{
		\frac{\pi(T^{\ast}_k,M^{\ast}_k)}{\#\{\mathcal{TN}(T^{\ast}_k)\}} 
		\prod_{i\in \mathcal{I}_{j}^k}c\left(u_{1i},u_{2i}\mid h\left(R_{ik}+\mu^{\ast}_j\right)\right) 
		\mathcal{N}\left(\mu_{jr};\mu^{\ast}_{jr},\gamma^{\ast}_{jr}\right)
		\mathcal{N}\left(\mu_{jl};\mu^{\ast}_{jl},\gamma^{\ast}_{jl}\right)}
	{\frac{\pi(T_k,M_k)}{\#\{\mathcal{PN}(T_k)\}} 
		\prod_{i\in \mathcal{I}_{jr}^k}c\left(u_{1i},u_{2i}\mid h\left(R_{ik}+\mu_{jr}\right)\right)
		\prod_{i\in \mathcal{I}_{jl}^k}c\left(u_{1i},u_{2i}\mid h\left(R_{ik}+\mu_{jl}\right)\right)
		\mathcal{N}\left(\mu^{\ast}_j;\mu_{j},\gamma_j\right)}.
\end{align*}

\paragraph{\textsc{change}/\textsc{swap} move:}
For the \textsc{change} or the \text{swap} move. The proposals only rely on the internal nodes and the terminal nodes and values remain unchanged. Moreover, for each of these moves $\mathcal{IN}(T_k)$ and $\mathcal{PCN}(T_k)$ remain invariant in nature. That is for a proposed tree $T_k^{\ast}$, $\mathcal{IN}(T_k)=\mathcal{IN}(T_k^{\ast})$ and $\mathcal{PCN}(T_k)=\mathcal{PCN}(T_k^{\ast})$. Therefore, the proposal ratios of these two moves are equal to 1. This gives the following:

\begin{align*}
	AR_{\gamma;\textsc{move}}\left(T_k,M_k;T_k^\ast, M_k^\ast\right)= \frac{
		\pi(T^{\ast}_k,M^{\ast}_k) 
		\prod_{i=1}^{n}c\left(u_{1i},u_{2i}\mid h\left(R_{ik}+g(x_i, T_k^\ast, M_k^\ast)\right)\right)}
	{\pi(T_k,M_k) 
		\prod_{i=1}^{n}c\left(u_{1i},u_{2i}\mid h\left(R_{ik}+g(x_i, T_k, M_k)\right)\right)}.
\end{align*}

Now, we show that our adaptive RJ-MCMC routine is ergodic. For that we first show that diminishing adaptation holds for our RJ-MCMC routine. For the proof of ergodicity the tree index $k$ plays no role so we drop it for notational convenience.

\subsection{Diminishing Adaptation}

Let $\left(T,M\right)$ be a state in the state space $\mathcal{T}\times \mathcal{M}$. Let $\Gamma^{\eta} \coloneqq \left(\Gamma_{1}^{\eta}, \cdots\Gamma^{\eta}_{n_L(T)}\right)$ be the adaption at the $\eta$-th iteration. Such that
\begin{equation*}\label{eq:var:adapt}
	\Gamma^{\eta}_{j} \coloneqq \frac{2.4^2}{\left(\#\left\{\mathcal{I}_{j}\right\}\right)^3}
	\sum_{c\in\mathcal{I}_{j}}\sum_{d\in\mathcal{I}_{j}}\left[C^{\eta}\right]_{cd}.
\end{equation*}
Then the following lemma holds

\begin{lemma}\label{lemm:bounded:ratio}
	Let $\delta>0$, then $\forall$ $(u_{1i},u_{2i})\in [\delta, 1-\delta]^2$,  the acceptance ratios are bounded. That is, there exist values $0\le b_{\gamma, \textsc{move}} <  B_{\gamma, \textsc{move}} \le \infty$ such that
	\begin{equation*}
		\frac{1}{B_{\gamma, \textsc{move}}} \le AR_{\gamma, \textsc{move}}\left(T_k,M_k;T_k^\ast, M_k^\ast\right) \le \frac{1}{b_{\gamma, \textsc{move}}}.
	\end{equation*}
\end{lemma}
This holds as the copula likelihood ratio becomes bounded in the compact set $[\delta, 1-\delta]^2$ and the adaptive proposal is bounded by construction leading to a bounded proposal ratio.

\begin{lemma}\label{lemm:stable:var}
	$\left\|\Gamma^{\eta+1}_{j} - \Gamma^{\eta}_{j}\right\|_{\infty}\to 0$ as $\eta\to\infty$.
\end{lemma}

\begin{proof}
	Recall the iterative variance formula given by \cref{eq:iter:var}. We define the increment by:
	\begin{equation*}
		\Delta C^{\eta + 1} \coloneqq \frac{1}{\eta}\left(\eta \left(\overline{V}_{\cdot }^{\eta-1}\right)\left(\overline{V}_{\cdot }^{\eta-1}\right)^T - (\eta+1)\left(\overline{V}_{\cdot }^{\eta}\right)\left(\overline{V}_{\cdot }^{\eta}\right)^T + \left({V}_{\cdot }^{\eta}\right)\left({V}_{\cdot }^{\eta}\right)^T + \epsilon\mathbf{I}_n\right)
	\end{equation*}
	and let, $ n_j = \#\left\{\mathcal{I}_{j}\right\}$ denote the number of observations contained in the $j$-th partition. Then
	\begin{align*}
		\Gamma^{\eta+1}_j 
		&= \frac{2.4^2}{\left(n_{j}\right)^3}
		\sum_{c\in\mathcal{I}_{j}}\sum_{d\in\mathcal{I}_{j}}\left[C^{\eta+1}\right]_{cd}\\
		&= \frac{2.4^2}{\left(n_{j}\right)^3}
		\left[\sum_{c\in\mathcal{I}_{j}}\sum_{d\in\mathcal{I}_{j}}\left[\frac{\eta-1}{\eta} C_k^{\eta} + \Delta C^{\eta + 1}\right]_{cd}\right] \\
		&= \frac{\eta-1}{\eta}
		\Gamma^{\eta}_j + \frac{2.4^2}{\left(n_{j}\right)^3} \left[\sum_{c\in\mathcal{I}_{j}}\sum_{d\in\mathcal{I}_{j}}\left[\Delta C^{\eta + 1}\right]_{cd}\right].
	\end{align*}
	Since the increment is of $O(1/\eta)$ in the $\eta$-th iteration, therefore $\left\|\Gamma^{\eta+1}_j - \Gamma^{\eta}_j\right\|_{\infty} \to 0$ as $\eta\to \infty$.
\end{proof}

Let $s = (T,M)$ and $s^\ast = (T^\ast,M^\ast)$ and the \textsc{grow} move is denoted by \textsc{g}. Then the kernel corresponding \textsc{grow} gives us
\begin{align*}
	&\left\|\mathcal{K}_{\Gamma_{\eta+1;\textsc{g}}}(s,\cdot)-\mathcal{K}_{\Gamma_{\eta};\textsc{g}}(s,\cdot)\right\|_{TV}\\
	&\le 2\int \left|\alpha_{\Gamma_{\eta+1};\textsc{g}}(s,s^\ast)q_{\Gamma_{\eta+1};\textsc{g}}(s,s^\ast) - \alpha_{\Gamma_{\eta};\textsc{g}}(s,s^\ast)q_{\Gamma_{\eta};\textsc{g}}(s,s^\ast)\right|ds^\ast\\
	&\le 2\int \left|\alpha_{\Gamma_{\eta+1};\textsc{g}}(s,s^\ast)\left(q_{\Gamma_{\eta+1};\textsc{g}}(s,s^\ast) - q_{\Gamma_{\eta};\textsc{g}}(s,s^\ast)\right)\right|ds^\ast 
	+ 2\int \left|\left(\alpha_{\Gamma_{\eta+1};\textsc{g}}(s,s^\ast)-\alpha_{\Gamma_{\eta};\textsc{g}}(s,s^\ast)\right)q_{\Gamma_{\eta};\textsc{g}}(s,s^\ast)\right|ds^\ast\\
	&\le 2\int \left|q_{\Gamma_{\eta+1};\textsc{g}}(s,s^\ast) - q_{\Gamma_{\eta};\textsc{g}}(s,s^\ast)\right|ds^\ast 
	+ 2\int \left|\alpha_{\Gamma_{\eta+1};\textsc{g}}(s,s^\ast)-\alpha_{\Gamma_{\eta};\textsc{g}}(s,s^\ast)\right|ds^\ast\int \left|q_{\Gamma_{\eta};\textsc{g}}(s,s^\ast)\right|ds^\ast\\
	&\le 2\int \left|q_{\Gamma_{\eta+1};\textsc{g}}(s,s^\ast) - q_{\Gamma_{\eta};\textsc{g}}(s,s^\ast)\right|ds^\ast + 2D_1\int \left|\alpha_{\Gamma_{\eta+1};\textsc{g}}(s,s^\ast)-\alpha_{\Gamma_{\eta};\textsc{g}}(s,s^\ast)\right|ds^\ast
\end{align*}
for some constant $0<D_1<\infty$ as the proposal is bounded by construction.

Since $q_{\Gamma_{\eta+1};\textsc{g}}(s,s^\ast)$ is a product of Gaussian probabilities with bounded variances and the adaption $\Gamma_{\eta+1}$ converges to $\Gamma_{\eta}$ by \cref{lemm:stable:var}, we can show that $\left\|q_{\Gamma_{\eta+1};\textsc{g}}(s,s^\ast)-q_{\Gamma_{\eta};\textsc{g}}(s,s^\ast)\right\|_{TV}$ goes to 0 as $\eta \to \infty$. Similarly, from \cref{lemm:bounded:ratio}, we know that the acceptance ratio is bounded and  $\alpha_{\Gamma_{\eta+1};\textsc{g}}(s,s^\ast)$ is trivially dominated by 1. Hence exploiting pointwise convergence of $\Gamma_\eta$ and continuity of $\alpha_{\Gamma_{\eta+1}}(s,s^\ast)$, we can show that $\left\|\alpha_{\Gamma_{\eta+1};\textsc{g}}(s,s^\ast)-\alpha_{\Gamma_{\eta};\textsc{g}}(s,s^\ast)\right\|_{TV}$ goes to 0 as $\eta \to \infty$. Hence $\left\|\mathcal{K}_{\Gamma_{\eta+1;\textsc{g}}}(s,\cdot)-\mathcal{K}_{\Gamma_{\eta};\textsc{g}}(s,\cdot)\right\|_{TV}$ goes to 0 as $\eta\to \infty$.

We can follow similar arguments for the \textsc{change} move to show that $\left\|\mathcal{K}_{\Gamma_{\eta+1;\textsc{c}}}(s,\cdot)-\mathcal{K}_{\Gamma_{\eta};\textsc{c}}(s,\cdot)\right\|_{TV}$ goes to 0 as $\eta\to \infty$. Therefore, we can show that the diminishing adaption holds for our proposed adaptive RJ-MCMC algorithm.

\subsection{Containment}

Recall the state space defined by $\mathcal{T}\times\mathcal{M}$. Then we can express this space as finite union of subspaces such that $\mathcal{T}\times\mathcal{M} \coloneq \bigcup_{i=1}^n \mathcal{T}_i \times \mathbb{R}^i$, where $\mathcal{T}_i$ is the space of all trees with $i$ terminal nodes and the vector of terminal nodes are in $\mathbb{R}^i$.

Let $T^{(i)}$ denote a tree with $i$ terminal nodes and $M^{(i)}$ denote the vector of terminal node values. Then for each $i=2,\cdots,n$, we have
\begin{align*}
	&\mathcal{K}_{\gamma}\left(T^{(i)},M^{(i)};\left(T^{(i-1)},M^{(i-1)}\right)\right)\\
	&\ge \alpha_{\gamma;\textsc{prune}}\left(T^{(i)},M^{(i)};T^{(i-1)},M^{(i-1)}\right)q_{\gamma;\textsc{prune}}\left(T^{(i)},M^{(i)};T^{(i-1)},M^{(i-1)}\right)d\left(M^{(i-1)}\right)\\
	&\ge \frac{1}{B_{\gamma;\textsc{prune}}}\frac{1}{\#\left\{\mathcal{PN}(T^{(i)})\right\}}\mathcal{N}\left(\mu_j^{*};\mu_{j},\gamma_j\right)d\mu_j^\ast.
\end{align*}
Therefore there exist $\nu>0$ such that for all $i=2,\cdots,n$
\begin{equation}\label{eq:reach:trivial}
	\mathcal{K}_{\gamma}\left(T^{(i)},M^{(i)};d\left(T^{(i-1)},M^{(i-1)}\right)\right)
	\ge \frac{\nu}{\#\left\{\mathcal{PN}(T^{(i)})\right\}}\mathcal{N}\left(\mu_j^{*};\mu_{j},\gamma_j\right)d\mu_j^\ast.
\end{equation}
Let $(T^{(1)},M^{(1)})$ denote the trivial tree. That is tree with only one node. Then for all $i$, we can reach the trivial tree with non-zero probability. Hence, we can define be a measure, $\phi$ such that $\phi\left(d\left(T^{(i)},M^{(i)}\right)\right) = \delta_{1}(i)$, where $\delta(\cdot)$ is Kronecker's delta. This shows that $\mathcal{K}(\cdot;\cdot)$ is $\phi$-irreducible. Now, let $\pi(\cdot\mid y)$ be the target density. Then by construction of the RJ-MCMC algorithm, $\pi(\cdot\mid y)$ is an invariant distribution of $\mathcal{K}(\cdot,\cdot)$. Therefore, $\mathcal{K}(\cdot,\cdot)$ is $\pi$-irreducible. Aperiodicity of the $\mathcal{K}(\cdot,\cdot)$ is also achieved through the construction. For instance, we can propose a pair $\left(T^{(2)},M^{(2)}\right)$ for which $\alpha_{\gamma,\textsc{grow}}\left(T^{(1)},M^{(1)}, T^{(2)},M^{(2)}\right) < 1$. Then the rejection probability $r_{\gamma,\textsc{grow}}\left(T^{(1)},M^{(1)}\right)$ of \textsc{grow} is given by:
\begin{align*}
	& r_{\gamma,\textsc{grow}}\left(T^{(1)},M^{(1)}\right)\\
	&=1-\int \alpha_{\gamma,\textsc{grow}}\left(T^{(1)},M^{(1)}, T^{(2)},M^{(2)}\right)q_{\gamma,\textsc{grow}}\left(T^{(1)},M^{(1)}, T^{(2)},M^{(2)}\right) d\left(T^{(2)},M^{(2)}\right)\\
	&=\int\left(1 - \alpha_{\gamma,\textsc{grow}}\left(T^{(1)},M^{(1)}, T^{(2)},M^{(2)}\right)\right) q_{\gamma,\textsc{grow}}\left(T^{(1)},M^{(1)}, T^{(2)},M^{(2)}\right) d\left(T^{(2)},M^{(2)}\right)\\
	& > \int q_{\gamma,\textsc{grow}}\left(T^{(1)},M^{(1)}, T^{(2)},M^{(2)}\right) d\left(T^{(2)},M^{(2)}\right) > 0.
\end{align*}
Therefore, there exists $1>\nu_0 >0$ such that
\begin{equation}\label{eq:stay:trivial}
	\mathcal{K}_{\gamma}\left(T^{(1)},M^{(1)};d\left(T^{(1)},M^{(1)}\right)\right) \ge \nu_0 = \nu_0 \phi\left(d\left(T^{(1)},M^{(1)}\right)\right).
\end{equation}

Now, to show that the Markov chain is uniformly ergodic we show that $\mathcal{T}\times \mathcal{M}$ is a small set. Then for $k=2,\cdots,n$, we iterate the kernel $n$ times and using Chapman Kolmogorov equation we get
\begin{align*}
	& \mathcal{K}_{\gamma}^n\left(T^{(k)},M^{(k)};d\left(T^{(1)},M^{(1)}\right)\right)\\
	& = \int_{\mathcal{T}\times\mathcal{M}}\mathcal{K}_{\gamma}^{k-1}\left(T^{(k)},M^{(k)};d\left(T,M\right)\right)\mathcal{K}_{\gamma}^{n+1-k}\left(T,M;d\left(T^{(1)},M^{(1)}\right)\right)\\
	& \ge \int_{\mathcal{T}_1\times\mathbb{R}}\mathcal{K}_{\gamma}^{k-1}\left(T^{(k)},M^{(k)};d\left(T,M\right)\right)\mathcal{K}_{\gamma}^{n+1-k}\left(T,M;d\left(T^{(1)},M^{(1)}\right)\right)\\
	& = \mathcal{K}_{\gamma}^{k-1}\left(T^{(k)},M^{(k)};d\left(T^{(1)},M^{(1)}\right)\right)\mathcal{K}_{\gamma}^{n+1-k}\left(T^{(1)},M^{(1)};d\left(T^{(1)},M^{(1)}\right)\right).
\end{align*}
Therefore, from \cref{eq:reach:trivial} and \cref{eq:stay:trivial}, we get
\begin{equation*}
	\mathcal{K}_{\gamma}^n\left(T^{(k)},M^{(k)};d\left(T^{(1)},M^{(1)}\right)\right) \ge \nu^{k-1} \nu_0^{n+1-k}\phi\left(d\left(T^{(1)},M^{(1)}\right)\right).
\end{equation*}
This holds true for $k=1$ as well. Therefore, for any $\left(T^{(k')},M^{(k')}\right)$ in $\mathcal{T}\times\mathcal{M}$

\begin{equation*}
	\mathcal{K}_{\gamma}^n\left(T^{(k)},M^{(k)};d\left(T^{(k')},M^{(k')}\right)\right) \ge \vartheta\phi\left(d\left(T^{(k')},M^{(k')}\right)\right),
\end{equation*}
where $\vartheta = \min_{1\le k\le n}\left\{\nu^{k-1} \nu_0^{n+1-k}\right\}$. Then following the proposition given by \citet{Tierney1994}, we can find a constant $L_0>0$ such that for all $N$
\begin{equation}
	\left\|\mathcal{K}^N_{\gamma}(T,M;\cdot) - \pi(\cdot\mid y)\right\|_{TV} \le L_0 (1-\vartheta)^{\lfloor N/n\rfloor}.
\end{equation}
Therefore, $K_{\gamma}(T,M;)$ is geometrically ergodic for all values of $\gamma$ and $(T,M)$. 

\section{Synthetic dataset}\label{app:results}

Additional results and figure relevant to our analyses with Frank copula family with sub optimal proposal variance.

\begin{figure}[H]
	\centering
	\includegraphics[width = 0.95\linewidth]{"frank_longer_sim.pdf"}
	\caption{Trace plots of the likelihood; the number of terminal nodes; and the depth of the tree for the Frank copula dataset using tree based Kendall's tau (\cref{eq:tree:tau}). The plots are obtained by running 4 parallel chains with 7500 MCMC iterations each. The left column shows analyses with C-BART and the right column shows analyses with A-C-BART. In this case the proposal variance is equal to $0.2$.}
	\label{fig:trace:frank}
\end{figure}


\begin{table}[H]
	\centering
	\begin{tabular}{l|cccccc}
		& Mean ($\hat{n}_L$) & SD ($\hat{n}_L$) & Mean ($\hat{D}$) & SD ($\hat{D}$) & Mean Acc. & SD Acc. \\ 
		\midrule
		Frank (C-BART) & 2.242 & 0.277 & 1.232 & 0.265 & 0.111 & 0.029 \\
		Frank (A-C-BART) & 2.592 & 0.343 & 1.569 & 0.329 & 0.117 & 0.024 \\
	\end{tabular}
	\caption{Efficiency of our proposed method for Frank copula dataset using a tree based conditional Kendall's tau (\cref{eq:tree:tau}). We present the average and standard deviation of the posterior expectation of the number of terminal nodes; the posterior expectation of the depth of the tree; and the acceptance rate in $[0,1]$ scale. The quantities are obtained by running 4 parallel chains with 7500 MCMC iterations each. In this case the proposal variance is equal to $0.2$.}
	\label{tab:eff:ex1:frank}
\end{table}

\begin{table}[H]
	\centering
	\begin{tabular}{l|ccc|ccc}
		\multicolumn{1}{c|}{} &
		\multicolumn{3}{c|}{C-BART} &
		\multicolumn{3}{c}{A-C-BART} \\
		\midrule
		& RMSE & CI-length & CI-cov & RMSE & CI-length & CI-cov \\ 
		\midrule
		Frank & 0.194 & 0.224 & 0.432 & 0.153 & 0.244 & 0.688 
	\end{tabular}
	\caption{Prediction accuracy of our proposed method for Frank copula dataset using a tree based conditional Kendall's tau (\cref{eq:tree:tau}). We split our results in two general columns: left of which is for C-BART, whereas the right one is for A-C-BART. We then create subcolumns under each column to present root mean squared error (RMSE); 95\% credible interval length (CI-length); and 95\% credible interval coverage (CI-cov). The quantities are obtained by running 4 parallel chains with 7500 MCMC iterations each. In this case the proposal variance is equal to $0.2$.}
	\label{tab:pred:ex1:frank}
\end{table}

\section{CIA world fact data}\label{app:cia}
Additional figures related to our analyses with 10 trees.
\begin{figure}[H]
	\centering
	\includegraphics[width = 0.95\linewidth]{"LE_vs_GDP_taus_10.pdf"}
	\caption{Estimated dependence between male life expectancy and female life expectancy conditional on scaled log-GDP. For modelling, we use 10 trees and run 4 parallel chains involving 30000 iterations. For posterior inference, we discard the first 5000 samples.}
	\label{fig:taus:LE10}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width = 0.95\linewidth]{"LE_vs_GDP_likelihood_10.pdf"}
	\caption{Trace plots of log-likelihood obtained from our analyses with life expectancy of male and female population. The plots are obtained by running 4 parallel chains each one with 30000 MCMC iterations and 10 trees. The left columns shows analyses with C-BART and the right column shows analyses with A-C-BART.}
	\label{fig:trace:like:real:LE10}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width = 0.95\linewidth]{"LE_vs_GDP_woa_10.pdf"}
	\caption{Scatter plots of pseudo observations and predicted copulas obtained for life expectancy of male and female population using C-BART with 10 trees.}
	\label{fig:pseudo:LE:woa10}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width = 0.95\linewidth]{"LE_vs_GDP_wa_10.pdf"}
	\caption{Scatter plots of pseudo observations and predicted copulas obtained for life expectancy of male and female population using A-C-BART with 10 trees.}
	\label{fig:pseudo:LE:wa10}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width = 0.95\linewidth]{"LT_vs_GDP_likelihood_10.pdf"}
	\caption{Trace plots of log-likelihood obtained from our analyses with literacy of male and female population. The plots are obtained by running 4 parallel chains each one with 30000 MCMC iterations and 10 trees. The left columns shows analyses with C-BART and the right column shows analyses with A-C-BART.}
	\label{fig:trace:like:real:LT10}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width = 0.95\linewidth]{"LT_vs_GDP_taus_10.pdf"}
	\caption{Estimated dependence between male literacy and female literacy, conditional on scaled log-GDP. For modelling, we use 10 trees and run 4 parallel chains involving 30000 iterations. For posterior inference, we discard the first 5000 samples.}
	\label{fig:taus:LT10}
\end{figure}
% 0.84
\begin{figure}[H]
	\centering
	\includegraphics[width = 0.95\linewidth]{"LT_vs_GDP_woa_10.pdf"}
	\caption{Scatter plots of pseudo observations and predicted copulas obtained for literacy of male and female population using C-BART with 5 trees.}
	\label{fig:pseudo:LT:woa10}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width = 0.95\linewidth]{"LT_vs_GDP_wa_10.pdf"}
	\caption{Scatter plots of pseudo observations and predicted copulas obtained for literacy of male and female population using A-C-BART with 5 trees.}
	\label{fig:pseudo:LT:wa10}
\end{figure}


\bibliographystyle{plainnat}
\bibliography{example}

\end{document}
