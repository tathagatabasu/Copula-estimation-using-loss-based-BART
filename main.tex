\documentclass{amsart}
\usepackage[foot]{amsaddr} % put addresses on first page

\usepackage{geometry}
\usepackage{booktabs}
\usepackage{graphicx,psfrag,epsf}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{amsmath}
\allowdisplaybreaks
\usepackage{longtable}
\usepackage{bigints}
\usepackage{siunitx}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{amsthm}
\usepackage{soul}
\usepackage{color}
\usepackage{hyperref}
\usepackage[capitalise]{cleveref}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}{Lemma}[theorem]
\newtheorem{definition}{Definition}[section]
%

\usepackage[numbers]{natbib}
\usepackage{url} % not crucial - just used below for the URL 
\usepackage{doi}

\newcommand{\dashrule}{\hdashline\noalign{\vskip 0.5ex}}

\title{Conditional Copula models using loss-based Bayesian Additive Regression Trees}
\author{Tathagata Basu$^1$}
\author{Fabrizio Leisen$^2$}
\author{Cristiano Villa$^3$}
\author{Kevin Wilson$^1$}
\address{$^1$Newcastle University, UK}
\address{$^2$Kings College London, UK}
\address{$^3$Duke Kunshan University, China}

\date{\today}

	\begin{document}

\begin{abstract}
	 We present a novel semi-parametric Bayesian approach for modelling conditional copulas to understand the dependence structure between two random variables when it is influenced by an external factor. We use Bayesian additive regression trees to model the conditional copulas. We exploit the recent advancement in loss based priors for the BART model, which is designed to reduce the loss in information and complexity for tree misspecification giving us a parsimonious model that avoids over-fitting, a common issue of BART models. We test our model with synthetic dataset to illustrate the capability of our model in estimating the conditional copula parameter and present a study involving real data to showcase its applicability.
\end{abstract}

\keywords{Conditional Copula; BART; Objective Bayes; Semi-parametric estimation}

\maketitle

\section{Introduction}
Modelling the dependence structure is an important problem in multivariate analysis. To model such dependence structures, we often require non-trivial multivariate distribution functions which makes this problem extremely challenging. Sklar's theorem\cite{sklar:1959} on the dependence structure simplified such tasks, which states that a multivariate distribution can be modelled using it's marginals and copula that is a joint cumulative distribution function on the unit hypercube. Following this, several other theoretical advancements (\citet{kimeldori1975uniform,ruschendorf1976,schweizer1981,Genest01091993,Genest1993mult}) in the field made copula inference popular in the context of statistical modelling leading to its prominence in applied statistical literature involving  survival analysis (\citet 	{clayton1978model,oakes1989bivariate,zheng1995estimates,shih1995inferences,braekers2005copula}); risk management (\citet{fama1993common,BURGERT2006289,engle1990asset}); engineering applications (\citet{salvadori2007use,aghakouchak2010copula}); genetics (\citet{li2006quantitative}) etc. We refer readers to \citet{GENEST2024105278}'s review on copula modelling in remembrance of Abe Sklar for a detailed literature review.

Despite the use of copulas in applied statistics, adjustment for confounding in copulas is a relatively new concept. \citet{patton2006} formalised the conditional version of Sklar's theorem for its applicability in financial time series modelling. Initial works on conditional copulas were mostly for estimating time varying dependence structures using likelihood based approaches for autoregressive models (\citet{patton2006,JONDEAU2006827,BARTRAM20071461}). Later \citet{acar2010} proposed a non-parametric approach for estimating conditional copula for general problems. Several other likelihood-based non-parametric (\citet{GIJBELS20111919,Gijbels2012mult_cop}) and semi-parametric (\citet{ABEGAZ201243}) approaches were proposed in this regard. Whereas \citet{valle_cond_cop} proposed a Dirichlet mixture models for estimating conditional copula and \citet{GRAZIAN2022107417} proposed an approximate Bayesian approach for the same. While the previously mentioned likelihood-based approaches rely on kernel estimators, \citet{BonacinaLopezThomas+2025} proposed a ``Classification and Regression Tree'' (CART) \cite{brei_CART} algorithm for modelling conditional copula and investigated consistency of the CART algorithm in conditional copula estimation.

The introduction of CART algorithm for conditional copula estimation motivates us to explore the viability of employing Bayesian additive Regression Trees (BART), the Bayesian alternative of CART introduced by \citet{chipman2010BART}. BART provides a generalisation of earlier known Bayesian CART models (\citet{chipman98BCART,denison98BCART}) where a single regression tree was used. Due to the flexible nature of BART, it has been explored in different contexts such as Poisson regression (\citet{Murray03042021}); survival analysis (\citet{Sparapani_BART}); gamma regression (\citet{Linero_BART_gamma}); generalised BART (\citet{Linero02012025}) etc. While the original tree prior proposed by \citet{chipman98BCART} is most commonly used in BART literatures, it is not very straightforward to incorporate prior information on the number of terminal nodes. Whereas, the prior proposed by \citet{denison98BCART} tends to produce skewed trees. Several alternatives were proposed (\citet{Wu_CART,rockova_BART,Linero_BART_VS}) to tackle this issue but choice of hyperparameters remains subjective. In order to reduce such subjectivity, \citet{serafini2024lossbasedpriortreetopologies} proposed a novel prior for regression tree using a loss based approach developed by \citet{villa_loss-prior}. The prior is formulated based on minimising the loss incurred due to the misspecification of the tree which considers both the loss in information and complexity of the tree, making it appealing for BART models.

In this article, we exploit the loss-based BART prior proposed \citet{serafini2024lossbasedpriortreetopologies} to propose a novel approach for modelling conditional copulas. However, unlike previous applications of BART models, we do not have a straight forward likelihood function to chose a conjugate prior. While, the method proposed by \citet{Linero02012025} is appealing for most problems, we noticed that a Laplace approximation based approach is not
viable for conditional copula modelling as the first and second order derivatives may not be numerically stable. So we propose a reversible jump MCMC algorithm to sample from the posterior. We also propose an adaptive routine for the proposal distribution that is designed to get a better posterior estimates of the terminal node values. Moreover, we notice from initial analyses, such adaptive approach also performs better in capturing the true parsimonious tree structure. We illustrate our method using different copula models to show the efficiency of our approach in identifying the true tree structure as well as estimating the true conditional dependence. However, we would like to note that due to its simple and fast implementation, it can easily be adapted for other modelling problems. 

The rest of the paper is organised as follows: in \cref{sec:prelim} we introduce preliminary concepts of conditional copulas and Bayesian additive regression trees followed by our semi-parametric estimation approach for conditional copulas. In \cref{sec:rjmcmc}, we discuss our proposed reversible jump MCMC routine for sampling from the posterior along with its adaptive variant. After that, we illustrate our method using synthetic dataset in \cref{sec:sim} to monitor it's efficiency and accuracy; followed by case studies involving real life dataset in \cref{sec:cia}. Finally, we discuss the results shown in the paper and conclude it in \cref{sec:conc}.

\section{Preliminaries}\label{sec:prelim}

In this section, we present a formal description of conditional copulas followed by the BART models and the loss-based prior for BART proposed by \citet{serafini2024lossbasedpriortreetopologies}, which we will incorporate for modelling.

\subsection{Conditional copula}
Let $Y_1$ and $Y_2$ be two continuous random variables and $X$ be a continuous random variable that might affect the relationship between $Y_1$ and $Y_2$. Let $H_x(y_1,y_2)$ denotes the joint distribution of $(Y_1,Y_2)$ conditional on $X$. Then, according to Patton's\cite{patton2006} interpretation of Sklar's\cite{sklar:1959} theorem, there exists a unique copula $C_x$ such that
\begin{equation*}
	H_x(y_1,y_2) = C_x(F_{1x}(y_1),F_{2x}(y_2))
\end{equation*}
where $F_{ix}(y_i)$ is the cdf of $Y_i$ conditional on $X$ for $i=1,2$. Alternatively, we can write the following form of copula distribution function given by:
\begin{equation*}
	C_x(u_1,u_2) = H_x\left(F_{1x}^{-1}(u_1),F_{2x}^{-1}(u_2)\right)
\end{equation*}
where $u_i = F_{ix}(y_i)$ are pseudo observations and $F_{ix}^{-1}(u_i)$s are conditional quantile functions
for $i=1,2$. For a more detailed introduction to the concept, we recommend the works of \citet{patton2006,acar2010,GIJBELS20111919} etc.

\subsection{Loss-based BART}

Let, $Z\coloneqq(Z_1,\cdots,Z_n)$ denote $n$ outputs and $x\coloneqq(x_1,\cdots,x_n)$ denote corresponding $p$-dimensional inputs. \citet{chipman2010BART} showed that we can approximate the functional relationship between $Z$ and $x$ using sum of regression trees given by:

\begin{equation}\label{eq:BART}
	Z_i \sim \mathcal{N}\left(\sum_{t=1}^m g(x_i, T_t, M_t),\sigma^2\right);\qquad 1\le i\le n.
\end{equation}
where $T_t$ denotes the $t$-th tree; $M_t$ denotes the vector of terminal node values $M_t =$ $\{\mu_1$,$\mu_2$, \dots, $\mu_{n_L(T_t)}\}$ of the $t$-th tree; $n_L(T_t)$ denotes the number of terminal nodes of the $t$-th tree; and $g(x_i, T_t, M_t)$ denotes the $t$-th regression tree.

Such representation allows us to approximate a function with piecewise constant values on a partitioned domain. The internal nodes of the tree create these partitions by assigning a splitting rule $x_{\cdot j}\le \tau$ for $1\le j \le p$ on each internal node where the value $\tau$ is either chosen from one of the observed values $x_{ij}$ or chosen uniformly in a range of values $(\underline{x}_{\cdot j},\overline{x}_{\cdot j})$. In order to be able to estimate the value at the terminal nodes, we want to ensure that at least one observation is associated with each terminal node. Such partitions are called \textit{valid}.  \citet{serafini2024lossbasedpriortreetopologies} introduce the following to formal definition in this regard.

\begin{definition}[Cell size] Given a partition $\mathbf{\Omega} = \{\Omega_k\}_{k=1}^N$ of $\mathcal{X} = [0,1]^p$ and a set of observations $x_1, x_2, \cdots, x_n$ such that $x_i\in \mathcal{X}$ for $i=1,2,\cdots, n$, the cell size $S(\Omega_k)$ of $\Omega_k$
	is the fraction of observations contained in $\Omega_k$.
	\begin{equation*}
		S(\Omega_k) = \frac{1}{n}\sum_{i=1}^n \mathbb{I}(x_i\in \Omega_k).
	\end{equation*}
\end{definition}

\begin{definition}[Valid partition]
	A partition is said to be valid if
	\begin{equation*}
		S(\Omega_k) \ge \frac{C^2}{n}, \quad\text{for any } k=1,2,\cdots, N
	\end{equation*}
	for a constant $C^2\ge 1$.
\end{definition}

The most used prior for tree topology was introduced by \citet{chipman98BCART} for a single tree. Afterwards \citet{chipman2010BART} extended the framework for sum of regression trees. Recently, \citet{serafini2024lossbasedpriortreetopologies} an alternate prior for the tree topology. They consider a loss-based approach \cite{villa_loss-prior} where the associated loss function for misspecification of a tree has two components: loss in information and loss in complexity. In doing so, they define the following hierarchical prior

\begin{align}\label{eq:L-BART}
	\begin{split}
		T_t, M_t &\sim \pi(T_t)\pi(M_t\mid T_t)\\
		T_t &\propto \exp\left(\omega n_L(T_t)-\gamma\Delta(T_t)\right)\\
		\pi(M_t\mid T_t) & = \prod_{j=1}^{n_L(T_t)}\pi(\mu_j\mid T_t).
	\end{split}
\end{align}
where $\Delta(T_t)$ is the difference between right terminal nodes and left terminal nodes of $t$-th given tree.


\section{Conditional Copula Modelling}\label{sec:cond:cop}

As hinted earlier, our main objective is to model the dependence structure of a conditional copula. While some works are done based on estimating the margins of the copula, our approach is focused on estimating the conditional copula parameter. To do so, we use a suitable link function $h$, to model the conditional copula parameter such that 

\begin{equation*}
	\theta(x_i) \coloneqq h\left(\sum_{t=1}^m g(x_i, T_t, M_t)\right);\qquad 1\le i\le n.
\end{equation*}
Now, let $c\left(u_1,u_2\mid \theta(x)\right)$ denote the conditional copula density function. Then employing the loss-based prior for tree introduced in \cref{eq:L-BART}, we can define the following hierarchical model 
\begin{align}\label{eq:bayes:hier}
	\begin{split}
		u_{1i},u_{2i} \mid \theta(x_i) & \sim c\left(u_{1i},u_{2i}\mid h\left(\sum_{t=1}^m g(x_i, T_t, M_t)\right)\right)\\
		T_t &\propto \exp\left(\omega n_L(T_t)-\gamma\Delta(T_t)\right)\\
		\pi(M_t\mid T_t) &\propto \prod_{j=1}^{n_L(T_t)}\pi(\mu_j\mid T_t)\\
		t & = 1,2,\cdots m.
	\end{split}
\end{align}

\subsection{Choice of link functions} The choice of link function to relate the tree structure with the conditional copula parameter is dependent on the family of copula. Use of such link function is not completely unusual in the context of conditional copula modelling. \citet{ABEGAZ201243,valle_cond_cop} used such functions for calibrating the parameter. In our case, we wish to keep the sum of trees flexible and it can take any value in $\mathbb{R}$. So we consider functions that can map from $\mathbb{R}$ to the domain of $\theta$. A relevant table provided in \cref{sec:sim} where we evaluate the performance of our approach for different copula families. 

Note that, one may also want to use link function on the conditional Kendall's $\tau$ such that
\begin{equation*}
	\theta(x_i) \coloneqq h'(\tau(x_i))\quad\text{and}\quad \tau(x_i) = \sum_{t=1}^m g'(x_i, T_t, M_t).
\end{equation*}
This will allow one to estimate the conditional Kendall's $\tau$ directly whilst modelling the copula parameter. However, we refrain from doing so as the domain of $h'$ may vary based on the family of copula. For instance, Gaussian copula can take any value of $\tau$ in $(-1,1)$ where as for Gumbel family we $tau$ can only have non-negative values.

\subsection{Choice of priors on $\mu_j$}
Earlier adaptions of BART models (\citet{chipman2010BART,Sparapani_BART,Murray03042021}) suggested a conjugate prior for the terminal node values $\mu_j$ for integrating it out. In our case, we lack that benefit so we consider default $\mathcal{N}(0,\sigma_{t}^2)$ on $\mu_j$ as suggested by \citet{chipman2010BART,Linero02012025} where $\sigma_{t}^2$ is variance specific to the $t$-th tree. For $\sigma_{t}^2$ we use a flat inverse-gamma prior.

Note that, similar to our arguments around the link function, we can also consider a transformed beta prior \cite{gokhale_prior_cor} for $\mu_j$ if we wish to model the copula parameter with a link function on Kendall's $\tau$.

\subsection{Choice of $m$} The choice of the total number of trees still remains an open problem. \citet{chipman2010BART} considered a default of 500 trees whereas \citet{Linero02012025} suggested to 200 trees for analysis. In our case, we follow the approach of \citet{serafini2024lossbasedpriortreetopologies}, we start with 10 trees and increase it by 10 to monitor the change in performance.

\section{RJ-MCMC for Parameter Estimation}\label{sec:rjmcmc}

In this section we provide the MCMC algorithm for sampling from the posterior. First, let $(U_1,U_2)$ denote the pseudo observations $\{(u_{1i},u_{2i}):1\le i \le n\}$ and $X$ denote the covariates. The hierarchical model described in \cref{eq:bayes:hier} gives us a posterior proportional to the following form:
\begin{align}
	\begin{split}
		\prod_{i=1}^{n}c\left(u_{1i},u_{2i}\mid h\left(\sum_{t=1}^m g(x_i, T_t, M_t)\right)\right)\prod_{t=1}^{m}\pi(T_t)\prod_{t=1}^{m}\left(\prod_{j=1}^{n_L(T_t)}\pi(\mu_j\mid T_t)\right)\prod_{t=1}^{m}\pi(\sigma_{t}).
	\end{split}
\end{align}
So, to sample from the posterior we use a backfitting algorithm proposed by \citet{chipman2010BART} where we sample $(T_k, M_k)$ conditional on the other $m-1$ pairs of $(T_t,M_t)$. To be more precise, in each MCMC iteration, we sample $(T_k, M_k)$ from the following conditional distribution: 
\begin{equation*}
	(T_k,M_k)\mid T_{-k},M_{-k}, \sigma^2_{k}, U_1, U_2, X
\end{equation*}
followed by a sample of the variance term $\sigma_{k}$ from the following conditional distribution
\begin{equation*}
	\sigma^2_{k} \mid (T_k,M_k),U_1,U_2,X.
\end{equation*}
So in order to implement the backfitting algorithm, we define 
\begin{equation*}
	R_{ik} = \sum_{t\not=k}g(x_i, T_t, M_t)\quad\text{and}\quad R_{\cdot k}\coloneqq(R_{1k},R_{2k},\cdots,R_{nk}).
\end{equation*}
Here $R_{ik}$ is similar to the notion of residual discussed in \citet{chipman2010BART, serafini2024lossbasedpriortreetopologies}. However, unlike their problems of interest we are not fitting directly against the outcome data. Instead we will add this residual term with terminal node values of the $k$-th regression tree which give us the following posterior of $(T_k,M_k)$ conditional on $R_{\cdot k}$ and $\sigma_{k}$
\begin{align}\label{eq:post:res}
	\begin{split}
		\pi(T_k,M_k \mid R_{\cdot k}, \sigma^2_{k}, U_1,U_2, X) &\propto \pi(T_k)\prod_{i=1}^{n}c\left(u_{1i},u_{2i}\mid h\left(R_{ik}+g(x_i, T_k, M_k)\right)\right)\prod_{j=1}^{n_L(T_k)}\pi(\mu_j\mid T_k)
	\end{split}
\end{align}
For standard BART model, the use of conjugate prior allows us to marginalise the likelihood with respect to $\mu\coloneqq\left(\mu_1,\mu_2,\cdots,\mu_{n_L(T_k)}\right)$ such that:
\begin{equation*}
	\mathcal{L}(U_1,U_2\mid X, T_k, \sigma^2_{k})\propto \bigint_{\mu}\left(\prod_{i=1}^{n}c\left(u_{1i},u_{2i}\mid h\left(R_{ik}+g(x_i, T_k, M_k)\right)\right)\prod_{j=1}^{n_L(T_k)}\pi(\mu_j\mid T_k)\right)d\mu.
\end{equation*}
Unfortunately, we do not have the conjugacy property in our case. So we use a reversible jump MCMC algorithm \cite{green_RJMCMC} to compute from the posterior given by \cref{eq:post:res}. We follow, a similar setup to that of \citet{Linero02012025} to build our algorithm. However, for tree space exploration, we consider the original tree steps suggested by \cite{chipman98BCART}.

\subsection{Proposal for RJ-MCMC}

Reversible jump MCMC algorithms is a common strategy for trans-dimensional cases. This allows us to work around the issue of not having a conjugate prior for the terminal node values and we can sample $(T_k,M_k)$ efficiently in each iteration. For that, we consider a proposal function to generate a new pair $\left(T_k^\ast, M_k^\ast\right)$ at $\gamma+1$ iteration given by:
\begin{align}\label{eq:prop}
	q\left(T_k^\ast, M_k^\ast \mid T_k^{\gamma},M_k^{\gamma}\right) = q\left( T_k^\ast\mid T_k^{\gamma}\right) q\left(M_k^\ast\mid T_k^\ast, T_k^{\gamma}, M_k^{\gamma}\right).
\end{align}
Here, $q\left( T_k^\ast\mid T_k^{\gamma}\right)$ denote the tree proposal as described by \citet{chipman98BCART}. They suggested four different tree steps.
\begin{itemize}
	\item \textsc{grow}: Randomly choose a terminal node and split it into two terminal nodes
	\item \textsc{prune}: Randomly choose a parent of terminal nodes and turn into a terminal node
	\item \textsc{change}: Randomly choose an internal node and assign a new splitting rule
	\item \textsc{swap}: Randomly choose a parent-child pair of internal node and swap their splitting rules
\end{itemize}
For a detailed discussion on the splitting rule, we suggest the readers to look into \citet{chipman98BCART}. 

The generation of new terminal node values rely on the proposed tree structure that we represent with $q\left(M_k^\ast\mid T_k^\ast, T_k^{\gamma}, M_k^{\gamma}\right)$. For obvious reasons, we only need to consider \textsc{grow} and \textsc{prune} for new terminal node values as we can update all the terminal node values of $T_k$ from the full conditional using a Metropolis Hastings step. Therefore, the proposal for terminal node values is given by:
\begin{itemize}
	\item For \textsc{grow} we consider the $j$-th leaf to be grown to $j_l$ and$j_r$ so,
	$q\left(M_k^\ast\mid T_k^\ast, T_k^{\gamma}, M_k^{\gamma}\right)$ = $\pi_{prop}(\mu_{j_l})*\pi_{prop}(\mu_{j_r})$
	\item For \textsc{prune} we consider the $j_l$th and $j_r$th leaves to be pruned then $q\left(M_k^\ast\mid T_k^\ast, T_k^{\gamma}, M_k^{\gamma}\right)$ = $\pi_{prop}(\mu_j)$
\end{itemize}
For the choice of proposal we consider a normal distribution with mean 0 and variance $\sigma_{\text{prop}}$. We notice that fixing $\sigma_{\text{prop}}$ is rather difficult. While eye-balling the likelihood give us some idea but due to slow mixing nature of RJ-MCMC it still exhibits high autocorrelation. So, we suggest an adaptive variance for the proposal. Our approach is motivated from the seminal work of \citet{haario_AMH} where they suggest the use of MCMC samples to update the co-variance. They also provided a simple updating formula that reduced the computation cost of the covariance matrix. However, in our case we need to adapt the method to facilitate the \textsc{grow} and \textsc{prune} moves. So we propose an adaptive covariance based on the partition of the predictor space in \cref{alg:ada:prop}.

\begin{algorithm}[H]
	\caption{Computation of adaptive proposal}\label{alg:ada:prop}
	\begin{algorithmic}[1]
		\State Perform $t_0$ iterations with a fixed variance.
		
		\For{$k =1, \cdots, m$}
		\State Collect MCMC samples for initial $t_0$ iterations such that for the $k$-th tree so that:
		\begin{equation*}
			V_{ik}^{t} = g(x_i,T_k^{t},M_k^{t})\quad 1\le i \le n; 1\le t \le t_0
		\end{equation*}
		
		\State Calculate sample covariance matrix :
		\begin{equation*}
			C_k^{t} \coloneqq Cov\left(V_{\cdot k}^{1},V_{\cdot k}^{2},\cdots,V_{\cdot k}^{t-1}\right) + \epsilon\mathbf{I}_n \quad \text{for } t>t_0
		\end{equation*}
		
		\State Identify the indices of the observations that is contained in the partition $\Omega_\ast$ corresponding to the new terminal node such that $\mathcal{I}\coloneqq\left\{i:x_i \in \Omega_\ast\right\}$
		
		\State Calculate the new variance corresponding to the new terminal node value such that
		
		\begin{equation}\label{eq:var:adapt}
			\sigma_{\text{prop}}^2 \coloneqq \frac{2.4^2}{(\#\{\mathcal{I}\})^3}\sum_{i\in\mathcal{I}}\sum_{j\in\mathcal{I}}\left[C_k^{t}\right]_{ij}
		\end{equation}
		
		\EndFor
		
		\State Update sample covariance using iterative formula such that
		\begin{equation*}
			C_k^{t+1} \coloneqq \frac{t-1}{t} C_k^{t+1} + \frac{1}{t}\left(t \left(\overline{V}_{\cdot k}^{t-1}\right)\left(\overline{V}_{\cdot k}^{t-1}\right)^T - (t+1)\left(\overline{V}_{\cdot k}^{t}\right)\left(\overline{V}_{\cdot k}^{t}\right)^T + \left({V}_{\cdot k}^{t}\right)\left({V}_{\cdot k}^{t}\right)^T + \epsilon\mathbf{I}_n\right)
		\end{equation*}
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
	\caption{One iteration of RJ-MCMC for copula BART}\label{alg:MCMC}
	\begin{algorithmic}[1]
		\State The previous steps gives us $(T_k^{\gamma},M_k^{\gamma})$
		\State Set $\theta(x_i) \leftarrow h\left(\sum_{t=1}^{m} g(x_i, T_t^{\gamma},M_t^{\gamma})\right)$ for $i = 1, \ldots, n$
		\For{$k = 1, \ldots, m$}
		\State Set $R_{ik} \leftarrow \sum_{t\not=k}g(x_i, T_t^{\gamma},M_t^{\gamma})$ for $i = 1, \ldots, n$
		
		\State Sample $(T_k^\ast, M_k^\ast)$ using $q\left(T_k^\ast, M_k^\ast \mid T_k^{\gamma},M_k^{\gamma}\right)$ in \cref{eq:prop} by randomly choosing between the \textsc{grow}, \textsc{prune}, \textsc{swap}, and \textsc{change} 
		
		\State Compute acceptance probability using $\alpha\left(T_k^{\gamma},M_k^{\gamma};T_k^\ast, M_k^\ast\right)$ in \cref{eq:acc:prob}.
		
		\State Set $(T_k^{\gamma+1}, M_k^{\gamma+1})=(T_k^\ast, M_k^\ast)$ with probability $\alpha\left(T_k^{\gamma},M_k^{\gamma};T_k^\ast, M_k^\ast\right)$ or $(T_t^{k+1}, M_t^{k+1})=(T_t^k,M_t^k)$.
		\State Set $\theta(x_i) \leftarrow h(R_{ik} + g(x_i, T_k^{\gamma+1}, M_k^{\gamma+1}))$ for $i = 1, \ldots, n$
		
		\State Sample $M_t$ from the full conditional using MH step once all the new trees are formed.
		
		\State Sample $\sigma_{k}^2$ from 
		\begin{equation*}
			\text{InvGamma}\left(a+\frac{n_L(T_k)}{2} , b + \frac{\sum_{j=1}^{n_L(T_k)}\mu_j^2}{2}\right)
		\end{equation*}
		\EndFor
	\end{algorithmic}
\end{algorithm}

The formulation in \cref{eq:var:adapt} ensures that the proposal variance is represents the variance of the mean of the observed values at each terminal nodes, which is how the traditional BART model updates the prior on the terminal node values.


Once we have a proposal for both the trees and terminal node values, we can define the acceptance probability in the following way:
\begin{equation}\label{eq:acc:prob}
	\alpha\left(T_k^{\gamma},M_k^{\gamma};T_k^\ast, M_k^\ast\right)
	= \frac{\mathcal{L}(T_k^\ast,M_k^\ast)\pi(T_k^\ast,M_k^\ast)q\left(T_k^{\gamma},M_k^{\gamma}\mid T_k^\ast, M_k^\ast\right)}
	{\mathcal{L}(T_k^{\gamma},M_k^{\gamma})\pi(T_k^{\gamma},M_k^{\gamma}) q\left(T_k^\ast, M_k^\ast \mid T_k^{\gamma},M_k^{\gamma}\right)}.
\end{equation}
where $\mathcal{L}(T_k,M_k)$ denotes the likelihood for a given pair of $(T_k,M_k)$.

Finally, once we have sampled a new $(T_k,M_k)$ we can employ a Metropolis within Gibbs to update $\sigma_{k}^2$ conditional on $(T_k,M_k)$. We provide one iteration of the RJ-MCMC algorithm in \cref{alg:MCMC}.


\section{Simulation Studies}\label{sec:sim}

In this section, we consider two synthetic settings to showcase the efficiency of our proposed approach in recovering the true parsimonious model as well approximating a complicated test function correctly. For both the setting, we investigate the performance of our method for five different copula families: Gaussian, Students t, Clayton, Gumbel and Frank. As hinted earlier, to model the conditional copula parameter, we consider different link functions which we present in \cref{tab:cop:link} below.

\begin{table}[H]
	\centering
	\begin{tabular}{l|c|c}
		Family & Support & Link function\\
		\midrule
		Gaussian & $\rho \in (-1,1)$ & $h(x)=2\text{sigmoid}(x) -1$\\
		Student-t & $\rho \in (-1,1)$ & $h(x)=2\text{sigmoid}(x) -1$\\
		Clayton & $\theta \in (0,\infty)$ & $h(x)=\exp(x)$\\
		Gumbel & $\theta\in [1,\infty)$ & $h(x)=\exp(x)+1$\\
		Frank & $\theta\in \mathbb{R}\neg \{0\}$ & $h(x)=x$\\
		\end{tabular}
	\caption{List of copula families used for our analyses followed by the support of conditional copula parameter and the corresponding link function which we will use to map from $\mathbb{R}$ to their support.}
	\label{tab:cop:link}
\end{table}

For data generation process, we sample $x_i$'s from a uniform distribution $U(0,1)$ for $1\le i\le n$. Afterwards we get our conditional Kendall's tau $(\tau(x_i))$ using following two equations.
\begin{equation}\label{eq:tree:tau}
	\tau_1(x_i) = \begin{cases}
		0.1 & x_i \le 0.33\\
		0.7 & 0.33 < x_i \le 0.66\\
		0.3 & 0.66 < x_i
	\end{cases}
\end{equation}
and
\begin{equation}\label{eq:sin:tau}
	\tau_2(x_i) = 0.4\sin(2\pi x_i) + 0.5.
\end{equation}
We use these $\tau(x_i)$'s to simulate copula data using the relevant functions from \texttt{VineCopula} package in \texttt{R}. Then we use these copula datasets to perform our posterior inference. For each dataset, we run 20 parallel chains comprising of 6000 MCMC iterations. We also check the performance of our RJ-MCMC algorithm with and without adaption. For adaptive version of our algorithm we use first 1000 iterations for adaption. For convenience, from now on, we will use C-BART for our RJ-MCMC algorithm without adaption and A-C-BART for the adaptive version of it.

One specific goal of our analyses is to understand the efficiency of our method in capturing the true structure for that we take consider the average posterior samples. Besides that. we are also interested in the predictive performance of our approach for that we consider RMSE, 95\% credible interval length and 95\% credible interval coverage. 

\subsection{Example with true tree structure} 

We use the first case ($\tau_1(x)$) to monitor the efficiency of our proposed approach in capturing the tree structure. So, for the analysis we consider only 1 tree. This way, we check how close our posterior estimates are from the true number of terminal nodes which is equal to 3 and true depth which equals to 2. We present the efficiency of our approach in \cref{tab:eff:ex1}. We notice that for both C-BART and A-C-BART our posterior estimates of the number of terminal nodes and depths are close to their true value. The slight deviation can be attributed to the initial tree exploration phase where the model tends have a more complex trees. We also notice that for all five families of copula, the use of adaptive proposal gives us better estimates than estimates obtained through C-BART. We can also see that with the adaptive proposal, the acceptance rate is slightly lower. This may look counter intuitive but this can be explained from the fact that with an adaptive proposal the sampler tends to reject \textsc{grow} and \textsc{prune} steps after it recovers the true tree structure. This phenomena can be better appreciated from \cref{fig:trace:nterm:ex1} and \cref{fig:trace:depth:ex1}. We can notice that tree exploration stabilises after about 2000 iterations when we are using A-C-BART, unlike the samples from C-BART. Additionally, from these two figures, we can also see that for Clayton copula and Frank copula, the traceplots of C-BART and A-C-BART are somewhat similar, which is also visible from the \cref{tab:eff:ex1} where the acceptance rate is equal upto 2 decimal places. 

\begin{table}[ht]
	\centering
	\begin{tabular}{l|ccc|ccc}
		\multicolumn{1}{c|}{} &
		\multicolumn{3}{c|}{C-BART} &
		\multicolumn{3}{c}{A-C-BART} \\
		\midrule
		& $\mathbb{E}(n_L\mid U_1,U_2)$ & $\mathbb{E}(D\mid U_1,U_2)$ & Acc. & $\mathbb{E}(n_L\mid U_1,U_2)$ & $\mathbb{E}(D\mid U_1,U_2)$ & Acc. \\ 
		\midrule
		Gaussian & 3.33 & 2.17 & 0.15 & 3.20 & 2.08 & 0.14 \\ 
		Student t & 3.16 & 2.07 & 0.15 & 3.11 & 2.03 & 0.14 \\ 
		Clayton & 3.28 & 2.14 & 0.15 & 3.25 & 2.09 & 0.15 \\ 
		Gumbel & 3.14 & 2.07 & 0.14 & 3.10 & 2.02 & 0.14 \\ 
		Frank & 3.14 & 2.04 & 0.14 & 3.12 & 2.02 & 0.14 \\ 
	\end{tabular}
	\caption{Efficiency of our proposed method for simulated dataset using a tree based conditional Kendall's tau (\cref{eq:tree:tau}). We split our results in two general columns: left of which is obtained without any adaptive step where as the right one is obtained after 1000 adaptive iterations. We split these big columns in three smaller columns where present posterior expectation of number of terminal nodes ($\mathbb{E}(n_L\mid U_1,U_2)$); posterior expectation of depth ($\mathbb{E}(D\mid U_1,U_2)$); and acceptance rate in $[0,1]$ scale.}
	\label{tab:eff:ex1}
\end{table}

\begin{figure}
	\centering
	\includegraphics[width = 0.95\linewidth]{"case_1_nterm.pdf"}
	\caption{Trace plots of terminal nodes obtained from our analyses with synthetic datasets generated using tree based Kendall's tau (\cref{eq:tree:tau}). The plots are obtained by running 20 parallel chains each one with 6000 MCMC iterations. The left columns denote analyses with our RJ-MCMC algorithm without any adaption and the right column shows analyses with adaptive RJ-MCMC. The horizontal dashed black line represents the number of terminal nodes (3) of the data-generating tree.}
	\label{fig:trace:nterm:ex1}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width = 0.95\linewidth]{"case_1_depth.pdf"}
	\caption{Trace plots of death obtained from our analyses with synthetic datasets generated using tree based Kendall's tau (\cref{eq:tree:tau}). The plots are obtained by running 20 parallel chains each one with 6000 MCMC iterations. The left columns denote analyses with C-BART and the right column shows analyses with A-C-BART. The horizontal dashed black line represents the depth (2) of the data-generating tree.}
	\label{fig:trace:depth:ex1}
\end{figure}

We present the trace plots of our log-likelihood in \cref{fig:trace:like:ex1}. We notice that likelihood quickly moves towards the true likelihood value shown by the black dashed line and stabilizes around that region. We see a high autocorrelation but it is not very concerning as all the 20 chains remain close towards the true likelihood value without diverging. Similar effect is also reported by \citet{Linero02012025} regarding the mixing of the likelihood. One major improvement that we can see in this regard is that all the chains are in agreement and do not get stuck in low posterior regions, a phenomena that can occur with tree priors \cite{chipman98BCART}. 

We also present the accuracy of our method in terms prediction in \cref{tab:pred:ex1}. For that we discard first 1000 MCMC samples to calculate root mean squared error, 95\% credible interval length and 95\% credible interval coverage. We notice no significant differences in performance of the approaches and conclude that they are in agreement.

\begin{table}[ht]
	\centering
	\begin{tabular}{l|ccc|ccc}
		\multicolumn{1}{c|}{} &
		\multicolumn{3}{c|}{C-BART} &
		\multicolumn{3}{c}{A-C-BART} \\
		\midrule
		& RMSE & CI-length & CI-cov & RMSE & CI-length & CI-cov \\ 
		\midrule
		Gaussian & 0.044 & 0.163 & 0.994 & \textbf{0.041} & 0.156 & 0.994 \\ 
		Student-t & 0.064 & 0.191 & \textbf{0.982} & \textbf{0.063} & 0.190 & 0.980 \\ 
		Clayton & \textbf{0.049} & 0.144 & 0.986 & 0.050 & 0.147 & \textbf{0.988} \\ 
		Gumbel & 0.044 & 0.153 & 0.998 & \textbf{0.042} & 0.153 & \textbf{1.000} \\ 
		Frank & \textbf{0.048} & 0.175 & 0.998 & 0.049 & 0.180 & 0.998 \\ 
	\end{tabular}
	\caption{Prediction accuracy of our proposed method for simulated dataset using a tree based conditional Kendall's tau (\cref{eq:tree:tau}). We split our results in two general columns: left of which is obtained without any adaptive step where as the right one is obtained after 1000 adaptive iterations. We then create subcolumns under each column to present root mean squared error (RMSE); 95\% credible interval length (CI-length); and 95\% credible interval coverage (CI-cov).}
	\label{tab:pred:ex1}
\end{table}

\begin{figure}
	\centering
	\includegraphics[width = 0.95\linewidth]{"case_1_likelihood.pdf"}
	\caption{Trace plots of log-likelihood obtained from our analyses with synthetic datasets generated using tree based Kendall's tau (\cref{eq:tree:tau}). The plots are obtained by running 20 parallel chains each one with 6000 MCMC iterations. The left columns denote analyses with C-BART and the right column shows analyses with A-C-BART. The horizontal dashed black line represents the log-likelihood of the data-generating tree which are 141 (Gaussian copula); 190 (Student-t copula); 193 (Clayton copula); 147 (Gumbel copula); and 134 (Frank copula).}
	\label{fig:trace:like:ex1}
\end{figure}

\subsection{Example with a general function}
We use the second case ($\tau_2(x)$) to check the prediction accuracy of our proposed approach in estimating the dependence structure that is highly non-linear with respect to the confounding variable. For illustration purpose we use 10 trees to model the conditional copula. To illustrate the prediction accuracy, we used RMSE, CI length and CI coverage like discussed before. We present the results in \cref{tab:pred:ex2}. We notice that there's a significant difference between the performance of C-BART and A-C-BART, unlike our analyses with $\tau_1(x)$. We notice that RMSE wise A-C-BART dominates C-BART for all the cases. In terms of CI coverage, A-C-BART performs significantly better than C-BART for Gumbel and Frank copula. C-BART tends to perform better for Gaussian and Student-t copula but performance of A-C-BART is not notably worse. We also plot these results in \cref{fig:trace:pred:ex2} where we present true values of Kendall's conditional tau with red points; posterior estimates of Kendall's tau with black line and credible interval with green lines.


\begin{table}[ht]
	\centering
	\begin{tabular}{l|ccc|ccc}
		\multicolumn{1}{c|}{} &
		\multicolumn{3}{c|}{C-BART} &
		\multicolumn{3}{c}{A-C-BART} \\
		\midrule
		& RMSE & CI-length & CI-cov & RMSE & CI-length & CI-cov \\ 
		\midrule
		Gaussian & 0.065 & 0.282 & \textbf{0.982} & \textbf{0.062} & 0.294 & 0.972 \\ 
		Student-t & 0.062 & 0.337 & \textbf{1.000} & \textbf{0.058} & 0.329 & 0.996 \\ 
		Clayton & 0.067 & 0.252 & 0.986 & \textbf{0.062} & 0.257 & \textbf{0.992} \\ 
		Gumbel & 0.057 & 0.245 & 0.910 & \textbf{0.055} & 0.257 & \textbf{0.936} \\ 
		Frank & 0.060 & 0.252 & 0.886 & \textbf{0.059} & 0.260 & \textbf{0.914} \\ 
	\end{tabular}
	\caption{Prediction accuracy of our proposed method for simulated dataset using a non-linear conditional Kendall's tau (\cref{eq:sin:tau}). We split our results in two general columns: left of which is obtained without any adaptive step where as the right one is obtained after 1000 adaptive iterations. We then create subcolumns under each column to present root mean squared error (RMSE); 95\% credible interval length (CI-length); and 95\% credible interval coverage (CI-cov).}
	\label{tab:pred:ex2}
\end{table}

% 293.6331 299.5395 335.3049 291.9339 241.1606

\begin{figure}
	\centering
	\includegraphics[width = 0.95\linewidth]{"case_2_pred.pdf"}
	\caption{Comparison of BART fit obtained from our analyses with synthetic datasets generated using a non-linear Kendall's tau (\cref{eq:sin:tau}). We fit the model using 10 trees. We run 20 chains in parallel with 6000 MCMC iterations. The left columns denote analyses with C-BART and the right column shows analyses with A-C-BART. The green line denote the 95\% credible interval; the black line denotes the posterior estimate of Kendall's tau; and the red points show the true values of conditional Kendall's tau defined in \cref{eq:sin:tau}. To obtain the posterior estimates we discard first 1000 MCMC sample.}
	\label{fig:trace:pred:ex2}
\end{figure}

Similar to our analyses with first case, we notice that both C-BART and A-C-BART is able to move towards the true likelihood which we present in \cref{fig:trace:like:ex2}. Suggesting the efficiency of our method in exploring the parameter space.

\begin{figure}
	\centering
	\includegraphics[width = 0.95\linewidth]{"case_2_likelihood.pdf"}
	\caption{Trace plots of log-likelihood obtained from our analyses with synthetic datasets generated using a non-linear Kendall's tau (\cref{eq:sin:tau}). The plots are obtained by running 20 parallel chains each one with 6000 MCMC iterations. The left columns denote analyses with C-BART and the right column shows analyses with A-C-BART. The horizontal dashed black line represents the log-likelihood of the data-generating tree which are 294 (Gaussian copula); 300 (Student-t copula); 335 (Clayton copula); 292 (Gumbel copula); and 241 (Frank copula).}
	\label{fig:trace:like:ex2}
\end{figure}



\section{CIA world fact data}\label{sec:cia}
Here we present a case studies with real dataset to discuss the applicability of our method.

\section{Conclusion}\label{sec:conc}


\bibliographystyle{plainnat}
\bibliography{example}

\end{document}
